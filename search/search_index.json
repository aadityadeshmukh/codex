{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hello! My name is Aditya and this is Codex. Explore curated insights and high-level overviews of tech industry essentials, crafted by me to empower your professional journey.</p>"},{"location":"#the-high-levels-topics-are","title":"The high levels topics are:","text":""},{"location":"#management","title":"Management","text":"<p>All things related to Management</p> <p>Explore \u2197</p>"},{"location":"#development","title":"Development","text":"<p>All things related to software development </p> <p>Explore \u2197</p>"},{"location":"#devops","title":"DevOps","text":"<p>All things related to DevOps</p> <p>Explore \u2197</p>"},{"location":"#technologies","title":"Technologies","text":"<p>All things related to Technologies</p> <p>Explore \u2197</p>"},{"location":"Agile/","title":"Agile Methodologies","text":""},{"location":"DevOps/","title":"Devops","text":""},{"location":"DevOps/#fundamentals","title":"Fundamentals","text":"<p>Explore \u2197</p>"},{"location":"DevOps/#cicd","title":"CI/CD","text":"<p>Explore \u2197</p>"},{"location":"DevOps/#iac","title":"IaC","text":"<p>Explore \u2197</p>"},{"location":"DevOps/ci-cd/","title":"Continuous Integration / Continuous Delivery","text":"<p>Continuous Integration and Delivery means continuously building, testing, and delivering your code, you can reap huge stability, speed, and flexibility benefits.</p> <p>Benefits of CI/CD:</p> Info <ul> <li>CI/CD systems can differ a lot depending on the technology</li> <li>However, there are 5 key benefits one can see from CI/CD:<ul> <li>Empowering teams: The pipeline is a self service system. This makes the system transparent and easy to use by anyone</li> <li>Cycle Time: The cycle time from code commit to production is reduced significantly</li> <li>Security issues: Reduces security issues as security compliance integrates each time</li> <li>Continuous practice: Release date is not a big event</li> <li>More value: Spend less time doing unplanned work and more time value added work</li> </ul> </li> <li>A typical CI/CD pipeline in practice:     <pre><code>    graph LR\n    A[Source Code] --&gt; B[Version Control];\n    B --&gt;C[Build Tool];\n    C --&gt; D[Build &amp; tests];\n    D --&gt; G[Artifact store];\n    G --&gt; H[Local Deployment]\n    H --&gt; I[Deployed and tested]\n    I --&gt; J{Testing successful?}\n    J --&gt; |YES| K[Production Deployment]\n    J --&gt; |NO| L[Go to start of cycle]</code></pre></li> </ul> <p>Sample CI/CD pipeline</p> Info <ul> <li>Version Control<ul> <li>Standard tool: Git</li> <li>Version Control best practices:<ul> <li>Always use version control</li> <li>Put everything in version control</li> <li>Commit often. small batches</li> <li>Succint Commit messages</li> <li>do not commit code whose build is failing</li> <li>Consider using master branch approach (TBD)</li> <li>precommit hooks for quality control</li> </ul> </li> </ul> </li> <li>Continuous Integration<ul> <li>Defacto app: Jenkins</li> <li>Best practices:<ul> <li>Start with a clean build</li> <li>Build to pass the coffee test</li> <li>Run tests locally before committing</li> <li>Do not commit to a build which is broken</li> <li>Do not leave the build broken</li> <li>Do not remove failing tests. remove them if they are not valid</li> <li>Use notifications to communicate build status</li> <li>Datadog can be used to co-relate errors to code changes</li> </ul> </li> </ul> </li> <li>Artifact Repositories<ul> <li>Examples: Nexus or Artifactory</li> <li>Benefits of packaging code as artifacts:<ul> <li>Composeability</li> <li>Reliability</li> <li>Security</li> <li>Shareability</li> </ul> </li> </ul> </li> <li>Testing &amp; Continuous Delivery:<ul> <li>Unit Testing: Run with builds. Eg. JUnit</li> <li>Integration Testing: Run to check integration of various components. Eg. ServerSpec</li> <li>E2E testing: Run to check from a user flow perspective. Eg. Selenium tests</li> <li>Test philosophy:<ul> <li>TDD</li> <li>BDD : Cucumber</li> <li>ATDD : Acceptance Test driven development</li> </ul> </li> <li>Application Deployment and release:<ul> <li>Deploy with same artifact, in the same way, in an identical environment</li> <li>Tool: Ansible</li> </ul> </li> </ul> </li> </ul>"},{"location":"DevOps/devops-fundamentals/","title":"Devops Fundamentals","text":"<ul> <li>DevOps stands for Development &amp; Operations. </li> <li>It means shared responsibilities between the development and operations teams.</li> <li>The development team work being aware of the challenges faced by operations and contribute and ops team work more like developers with proper flow and process.</li> <li>DevOps is not a framework or a workflow. </li> <li>It's a culture that is overtaking the business world. </li> <li>DevOps ensures collaboration and communication between software engineers (Dev) and IT operations (Ops). With DevOps, changes make it to production faster, Resources are easier to share, And large-scale systems are easier to manage and maintain.</li> </ul> <p>Basics</p> Info <ul> <li>Values:<ul> <li>C (Culture) - How people interact</li> <li>A (Automation) - Automation at the heart of solution to the problem</li> <li>M (Measurement) - What to measure and incentivize accross the organization</li> <li>S (Sharing) - Feedback loops for discrete regular improvements based on transparency</li> </ul> </li> <li>Principles:<ul> <li>Systems Thinking<ul> <li>Consider the outcome of the entire pipeline or value chain</li> <li>For example adding app servers might overload the db with connections</li> <li>In case of IT the process might be helpful for the sub org but making the delivery slow</li> <li>Systems thinking must be used as guidance to set proper success criteria and evaluation of the system </li> </ul> </li> <li>Amplify Feedback loops<ul> <li>Effective feedback is what drives any control loop designed to improve the process</li> <li>Use amplify feeback loops to help when creating multi-team processes, visualizing metrics &amp; designing delivery flows</li> </ul> </li> <li>Continuous experimentation and learning<ul> <li>Focus on doing rather than talking about it</li> <li>Team must be ready to learn new things and the best way is to try to see if it works</li> <li>Use the approach to define team processes and standards, and as part of the leadership style</li> </ul> </li> </ul> </li> <li>Playbook (set of methodologies):<ul> <li>People over process over tools<ul> <li>Choose people, then define process and then choose the tools and not the other way around</li> </ul> </li> <li>Lean management<ul> <li>Work in small batches</li> <li>Work within progress limits</li> <li>Feedback loops</li> <li>Visualization</li> </ul> </li> <li>Continuous delivery<ul> <li>Code and release code regularly and in small batches</li> </ul> </li> <li>Visible Ops-style change control:<ul> <li>Eliminate fragile artifacts</li> <li>Create repeatable build process</li> <li>Manage dependencies</li> <li>Create and env of continuous improvement</li> </ul> </li> <li>Infrastructure as Code<ul> <li>System can and should be treated as code</li> <li>This standardizes the infrastructure and reduces the effort</li> </ul> </li> </ul> </li> <li>Practices:<ul> <li>Incident management system</li> <li>Devs on call</li> <li>Public status pages</li> <li>Blameless postmortems</li> <li>Embedded teams</li> <li>The cloud</li> <li>Andon cord</li> <li>Dependency Injection</li> <li>Blue green deployment</li> <li>Chaos monkey</li> </ul> </li> <li>Tools must be:<ul> <li>Programmable</li> <li>Verifyable</li> <li>Well behaved with the other parts of the system</li> </ul> </li> </ul> <p>DevOps: Culture problem</p> Info <ul> <li>When development and operations start working in silos problems begin to arise</li> <li>Problem is both groups are incentivised differently</li> <li>Both groups optimise their flows but it creates a less efficient overall system</li> <li>This needs to be solved by a culture shift</li> <li>Ways to do it:<ul> <li>Communication (Blameless postmortems):<ul> <li>Have a postmortem within 24-48 hours of the outage</li> <li>Build a timeline of the events</li> <li>Analyse the issues and discuss possible solutions</li> <li>Discuss how the customers were affected</li> <li>Document the learnings</li> <li>Discuss how can detection be done earlier in similar cases in the future</li> <li>Optimize for failure and recover than just prevention</li> </ul> </li> <li>Communication (Transparent uptime):<ul> <li>Admit failure</li> <li>Have an open communication channel</li> <li>Be authentic</li> <li>Have a POC</li> </ul> </li> <li>Collaboration:<ul> <li>Have a team that has people working on both dev and ops aspects</li> <li>Practice openness:<ul> <li>Open chatrooms</li> <li>Open wiki pages etc.</li> </ul> </li> </ul> </li> <li>Management best practices:<ul> <li>Cross functional teams</li> <li>Help people through the change</li> <li>Use Lean Agile processes</li> </ul> </li> <li>Kaizen (Continuous Improvement):<ul> <li>Principles:<ul> <li>Good process bring good results</li> <li>Go see for yourself</li> <li>Speak with data manage by facts</li> <li>Take action to correct and contain root causes</li> <li>Work as a team</li> <li>Its everyone's business</li> </ul> </li> <li>5 Whys<ul> <li>Ask questions in repeated iterations</li> <li>Do not accept time constraints as an answer find out what lead to the delay</li> <li>Do not accept manual failure as answer find out what process failed</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Building blocks of DevOps</p> Info <ul> <li>The main building blocks of DevOps are:</li> <li>Agile<ul> <li>DevOps is deep rooted in Agile</li> <li>Its highky suggested DevOps be implemented in conjunction with Agile as they are highly complimentary</li> <li>DevOps has roots in Agile and the process are iterative which generates quick product or solution delivery.</li> </ul> </li> <li>Lean<ul> <li>Principles:<ul> <li>Eliminate waste</li> <li>Amplify learning</li> <li>Decide as late as possible</li> <li>Decide as fast as possible</li> <li>Empower the team</li> <li>Build integrity</li> <li>See the whole</li> </ul> </li> <li>Techniques:<ul> <li>Kaizen</li> <li>Value-Stream mapping</li> </ul> </li> </ul> </li> <li>ITIL, ITSM and SDLC<ul> <li>These are prescriptive models mostly predecessors of modern day DevOps</li> </ul> </li> </ul> <p>Infrastructure as Code</p> Info <ul> <li>Infrastrucure as Code is a complete programmatic approach to infrastructure management</li> <li>It allows to manage infrastructure with the same principles as software development</li> <li>With IaC we can code the scripts in an IDE, run tests, apply decision making based on state and deploy automatically</li> <li>For example, we can completely describe an AWS system as code using a format called cloud formation which enables to replicate the system all the time</li> <li>Configuration Management:<ul> <li>Concepts:<ul> <li>Provisioning: Process of making a server ready for operation using hardware, OS, system drivers &amp; network connectivity</li> <li>Deployment: Automatically deploying and upgrading applications on a server</li> <li>Orchestration: Co-ordinating operations within multiple systems</li> <li>Configuration management: Overarching term for management of change control for system configuration after initial provision</li> </ul> </li> <li>Techniques - how tools approach configuration management<ul> <li>Imperative / procedural: Commands necessary to produce a state and defined and executed</li> <li>Functional / Declarative: We define the state and the tool converges the exisiting configuration based on the desired state</li> <li>Idempotent: Repeat execution equals same exact model</li> <li>Self-service: No need for manual intervention other than the requesting user</li> </ul> </li> </ul> </li> <li>Common toolchain:<ul> <li>For AWS: provisioning can be done via AWS cloud formation</li> <li>For Azure: Azure resource manager</li> <li>Terraform: Allows to provision in a more abstract way which can be translated to multiple platforms</li> </ul> </li> </ul> <p>Continuous Integration\\Delivery</p> Info <ul> <li>Continuous Integration: Automatically building and unit testing the entire application at regular intervals ideally at each code check-in</li> <li>Continuous Delivery: Automatically deploying every change to a production like environment and performing integration and acceptance testing</li> <li>Continuous Deployment: After automated testing automatically deploying the change to production</li> <li>Advantages:<ul> <li>Decreased time to market</li> <li>Quality increase</li> <li>Go live is not an event</li> <li>Lead time for changes is reduced</li> <li>State of Devops: having short lived feature branches and having less than 3 overall branches improves efficiency</li> <li>Lower mean time to recover</li> </ul> </li> <li>CI practices:<ul> <li>Short build times. Coffee test</li> <li>Commit really small bits</li> <li>Don't leave the build broken</li> <li>Trunk based development flow</li> <li>Don't allow flaky tests</li> <li>The build must return a status, log and artifact marked with the build number</li> </ul> </li> <li>CD practices:<ul> <li>No separate artifact for different environments</li> <li>Artifacts should be immutable</li> <li>Staging should be a copy of production</li> <li>Stop deployes if a previous step fails - (Andon cord)</li> <li>Deployments should be idempotent</li> </ul> </li> </ul> <p>Reliability Engineering:</p> Info <ul> <li>Ability of a system or component to function for a specified period of time</li> <li>MTTR: Mean time to recovery</li> <li>MTBF: Mean time between failures</li> <li>Reliability engineering typically involves embedding product knowledge into operations and operational knowledge into product</li> <li>Design for Operation:<ul> <li>Use design patterns (Gang of 4)</li> <li>Use reliability patterns like circuit breaker (Netflix, Hysterix library)</li> <li>12 factor app</li> <li>The success of the overall app relies on using the right patterns at the very beginning of the toolchain</li> <li>Have operational intelligence with the development capabilities</li> <li>This will provide better code shipped which is resilient and performant</li> <li>Netflix's chaos monkey actively kills servers and developers need to factor in this when they create applications</li> </ul> </li> <li>Operate for design:<ul> <li>Use a lean approach to monitoring and metrics</li> <li>Build a MVP, target a few systems, learn, repeat and go deeper as needed</li> <li>Build just enough metrics to gain insights and not overload the systems</li> <li>Areas for monitoring service uptime, application uptime, security, system usage, etc.</li> <li>Have a minimal centralized logging mechanism</li> <li>SRE toolchain:<ul> <li>Monitoring: Grafana, Containers (Prometheus)</li> <li>Logging: Splunk, ELK stack (Elasticsearch, logstash, kibana)</li> <li>Statuspage.io provides status pages as a service</li> <li>Security - Checkmarx (FOSS scans)</li> </ul> </li> </ul> </li> </ul>"},{"location":"DevOps/devops-project/","title":"Portfolio project: Explore California DevOps","text":""},{"location":"DevOps/devops-project/#a-project-implemented-using-key-devops-principles-such-as","title":"A project implemented using key DevOps principles such as:","text":"<ul> <li>Containerization with Docker</li> <li>Automated testing with RSpec, Capybara, &amp; Selenium</li> <li>Infrastructure as Code with Terraform</li> <li>AWS</li> <li>CI/CD as code with Jenkins</li> </ul>"},{"location":"DevOps/devops-project/#problem-statement","title":"Problem statement:","text":"<p>Explore California is a (fictitious) website already catering to a few hunder thousand customers and contributes significantly to the 5B USD. They face some issues and they want to fix them with DevOps. Analyse the problems and fix them with DevOps principles.</p>"},{"location":"DevOps/devops-project/#analysis","title":"Analysis","text":"Current state <ul> <li>The initial state of the app:<ul> <li>Production deploys are fragile and manual</li> <li>Development, UAT, production envs are very different</li> <li>Testing is limited</li> <li>getting them working on dev laptops is a chore</li> </ul> </li> </ul> Final state <ul> <li>The final expected state of the app:<ul> <li>Automated CI</li> <li>Automated production deploys</li> <li>Consistent environments</li> <li>Devs to be able to use their laptops for development</li> </ul> </li> </ul>"},{"location":"DevOps/devops-project/#high-level-design","title":"High level Design","text":"<ul> <li> <p>Full automation:</p> <ul> <li>The entire pipeline from code change to deployment should be automated with CI and CD</li> <li>The CD pipeline must deploy the website artifacts into AWS using terraform</li> <li>Unit tests must be integrated with CI and run before each PR goes into review</li> <li>Only code review and UAT must have a manual sign-off</li> </ul> </li> <li> <p>Containerization using docker for uniform environments</p> </li> </ul>"},{"location":"DevOps/devops-project/#implementation","title":"Implementation","text":""},{"location":"DevOps/devops-project/#testing-the-app-locally-with-docker-and-docker-compose","title":"Testing the app locally with Docker and Docker Compose","text":"<ul> <li>Setup docker for the legacy website which is based on nginx</li> <li> <p>The docker file will contain the following:</p> <p><pre><code>FROM nginx:alpine\nCOPY website /website\nCOPY nginx.conf etc/nginx/nginx.conf\nEXPOSE 80\n</code></pre> - Build and run the docker file:  <pre><code>  docker build --tag &lt;name&gt; &lt;dir&gt;\n  docker run --publish &lt;host port&gt;:&lt;docker port&gt; &lt;name&gt;\n</code></pre> - Docker compose can be used to bring all the services up in unison with networking. - Docker compose file is as follows: <pre><code>version: '3.7'\nservices:\nwebsite:\nbuild:\n    context: .\nports:\n    - 80:80\n</code></pre></p> </li> <li> <p>Bring the services up with the <code>docker-compose up</code> command</p> </li> <li>This will bring the site up on <code>localhost:80</code></li> <li>Check all the services currently running using the <code>docker-compose ps</code> command</li> <li>Bring the services down with the <code>docker-compose down</code> command</li> </ul>"},{"location":"DevOps/devops-project/#unit-testing-setup","title":"Unit testing Setup","text":"<ul> <li>In addition to the <code>website</code> service we need to setup a unit-tests service in docker compose yml</li> <li>It will run use capybara for authoring test cases and selenium web drivers for invoking a real browser</li> <li>The tests will be written in a declarative ruby rspec framework</li> </ul>"},{"location":"DevOps/devops-project/#setting-up-and-running-a-basic-test","title":"Setting up and running a basic test","text":"<ul> <li>Testcase:     <pre><code>require 'capybara'\nrequire 'capybara/dsl'\n\ndescribe \"Example website render unit tests\" do\n    it \"should show logo on the landing page\" do\n    end\nend\n</code></pre></li> <li>Dockerfile to install ruby, capybara and selenium:     <pre><code>FROM ruby:alpine\n\nRUN apk add build-base ruby-nokogiri\nRUN gem install rspec capybara selenium-webdriver\n\nENTRYPOINT [ \"rspec\" ]\n</code></pre></li> <li>Complete docker-compose yml file:     <pre><code>version: '3.7'\nservices:\nwebsite:\nbuild:\n    context: .\nports:\n    - 80:80\nunit-tests:\n    build:\n    dockerfile: rspec.dockerfile\n    context: .\n    volumes:\n    - $PWD:/app\n    command:\n    - --pattern\n    - /app/spec/unit/*_spec.rb\n</code></pre></li> <li>To run the unit tests first invoke the <code>website</code> service using <code>docker-compose up -d website</code></li> <li>Then run the <code>unit-tests</code> service <code>docker-compose run --rm unit-tests</code></li> </ul>"},{"location":"DevOps/devops-project/#demo","title":"Demo","text":""},{"location":"DevOps/iac/","title":"Infrastructure as Code","text":"<p>Infrastructure automation\u2014transitioning an organization's system administration from hardware into code\u2014is one of the major DevOps practice areas. By automating configuration management, you can make your systems more reliable, processes more repeatable, and server provisioning more efficient.</p>"},{"location":"Development/","title":"Development","text":"<p>All the knowledge related to development topics.</p>"},{"location":"Development/#sql","title":"SQL","text":"<p>Explore \u2197</p>"},{"location":"Development/#postgresql","title":"PostgreSQL","text":"<p>Explore \u2197</p>"},{"location":"Development/#django","title":"Django","text":"<p>Explore \u2197</p>"},{"location":"Development/#python-db-api","title":"Python-db API","text":"<p>Explore \u2197</p>"},{"location":"Development/django/","title":"Django","text":"Quick Start <ul> <li>Create a new directory and change to it</li> <li>Create a virtual environment using <code>python -m venv venv</code></li> <li>Activate the virtual env using <code>venv\\Scripts\\activate</code></li> <li>Install latest Django using <code>pip install django</code></li> <li>Post installation create a django project using <code>django-admin startproject smartnotes .</code></li> <li>To run this boilerplate project just run <code>python manage.py runserver &lt;port&gt;</code></li> <li>The project should start serving on localhost on the port provided</li> </ul> Expanding the project <ul> <li>Django projects are organized by apps</li> <li>Apps can be created using <code>django-admin startapp &lt;appname&gt;</code></li> <li>This will create a '' folder in the project which will have multiple files <li>To add a basic view add the final state of the views.py file should be:     <pre><code>    from django.shortcuts import render\n    from django.http import HttpResponse\n\n    def home(request):\n        return HttpResponse('Hello World!')\n</code></pre></li> <li>The request is sent by the urls.py from the root project. Its final state should be:     <pre><code>    from django.contrib import admin\n    from django.urls import path\n\n    from home import views\n\n    urlpatterns = [\n        path('admin/', admin.site.urls),\n        path('home', views.home),\n    ]\n</code></pre></li> <li>The default localhost page will start returning a 404 now but new end points will be added one of them would be the app name</li> <li>The views are served at that end point, example: <code>http://127.0.0.1:8001/home</code></li> <li>Django uses a framework called MVT i.e. Model view template</li> <li>In order to pass a proper html to the app we need to create a template directory inside the appname directory and create another appname directory inside templates directory</li> <li>In the templates/appname directory add the html file and include the required markup</li> <li>Change the views.py file to render this html page by returning the render function output: <code>render(request, 'appname/home.html', {'today', datetime.today()})</code></li> <li>The braces at the end is a way to pass on information to the view which can be accessed on the front end using <code>{{{today}}}</code></li> <li>Django projects are designed to be modular. The apps should be organized in a way that even if one app is deleted the project should not go down.</li> <li>In order to do this apps should be self contained and have just 1 concern</li> <li>Best practice is to have urls.py in each app which handles the routing and use include function in the project urls.py</li> Built-in user management <ul> <li> <p>Django Admin:</p> <ul> <li>To activate the admin interface you do not need to do anything special. Admin is alreay implemented.</li> <li>You need to create a superuser though to access it.</li> <li>First migrate the database using <code>python manage.py migrate</code></li> <li>Then simply run <code>python manage.py createsuperuser</code> to create a super user. The app will ask questions and create a user based on responses</li> <li>Access admin panel using the credentials</li> <li>By default the django admin provides way to create users and specify passwords and set previliges</li> <li>Any changes here is stored in the database</li> </ul> </li> <li> <p>Simple user authentication based access</p> <ul> <li>In order to restrict access to authorized users we can simply do the below in views.py:     <pre><code>    from django.contrib.auth.decorators import login_required\n    @login_required(login_url='/admin')\n    def authorized(request):\n        return render(request, 'home/auth.html', {})\n</code></pre></li> <li>In addition to the view we need to update the view in the urls.py:     <pre><code>    urlpatterns = [\n        path('home', views.home),\n        path('authorized', views.authorized),\n    ]\n</code></pre></li> <li>That's about it</li> </ul> </li> </ul> Connecting databases <ul> <li> <p>Creating the ORM model</p> <ul> <li>The sequence of flow that Django follows to create a model is: <ul> <li>Create ORM model in models.py of the app</li> <li>Run <code>Makemigrations</code></li> <li>Run <code>migrations</code></li> </ul> </li> <li>ORMs basically allow us to configure a object oriented model of the RDBMs</li> <li>In the models.py define a class where the class name should be the name of the table &amp; attribute to be name of the columns</li> <li>A typical class can be: <pre><code>from django.db import models\n\n# Create your models here.\nclass Notes(models.Model):\n    title = models.CharField(max_length=200)\n    note = models.TextField()\n    created = models.DateTimeField(auto_now_add=True)\n</code></pre></li> <li>After creating this run <code>python manage.py makemigrations</code> to create the migrations</li> <li>After this run <code>python manage.py migrate</code> to migrate the ORMs</li> </ul> </li> <li> <p>Exposing the model via Django Admin:</p> <ul> <li>Choose the admin.py from the app</li> <li>Override the <code>admin.ModelAdmin</code> class with the ModelAppname class</li> <li>either return a pass in case no extra customization is needed or make customizations</li> <li>Register this admin </li> <li>The sample class is as follows:     <pre><code>from django.contrib import admin\n\nfrom . import models\n\nclass NotesAdmin(admin.ModelAdmin):\n    list_display = ('title',)\n# Register your models here.\n\nadmin.site.register(models.Notes, NotesAdmin)\n</code></pre></li> <li>Once saved the model will be available from the django admin menu</li> </ul> </li> <li> <p>Adding and querying data from Django shell:</p> <ul> <li>Invoking Django shell: <code>python manage.py shell</code></li> <li>Import the model for the app e.g. <code>from notes.model import Notes</code></li> <li>In the shell use <code>notes = Notes.objects.all()</code> to get a QueryList of all notes in the db</li> <li>In the shell use <code>new_note = Notes.objects.create(title='', note='')</code> to create a new note</li> <li>In the shell use <code>Notes.objects.filter(title__startswith = 'My')</code> to filter during querying</li> </ul> </li> </ul> Building dynamic webpages <ul> <li>Showing data from the database on html pages:<ul> <li>We need to make changes to views.py and urls.py in the app</li> <li>We need to change the urls.py from the project</li> <li>We need to add a template html file</li> </ul> </li> </ul> <p>Example code: appname/views.py <pre><code>from django.shortcuts import render\nfrom .models import Notes\n# Create your views here.\n\ndef list(request):\n    list_notes = Notes.objects.all()\n    return render(request, 'notes/smart_notes.html', {'notes':list_notes})\n</code></pre></p> <p>Example code: appname/urls.py <pre><code>from django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('notes', views.list)\n]   \n</code></pre> Example html code to read data: <pre><code>&lt;body&gt;\n    &lt;ul&gt;\n        {% for note in notes %}\n            &lt;li&gt;{{note.title}}&lt;/li&gt;\n        {% endfor %}\n    &lt;/ul&gt;\n&lt;/body&gt;\n</code></pre></p> <p>Class Based Views:     - Class based views are alternative way to create views:     - Example class view:     <pre><code>from django.contrib.auth.mixins import LoginRequiredMixin\nfrom django.views.generic import TemplateView\n\n# simple class view\nclass HomeView(TemplateView):\n    template_name = 'home/welcome.html'\n    extra_context = {'today': datetime.today()}\n\n# authorized view\nclass AuthorizedView(LoginRequiredMixin, TemplateView):\n    template_name = 'home/auth.html'\n    login_url = '/admin'\n</code></pre></p> Building a front end <ul> <li> <ul> <li>Create a directory called \"static\" on the root level</li> <li>You need to tell Django that this will host the static files</li> <li>The way to do it is the settings.py</li> <li> <p>The settings.py should have the following setup:</p> <pre><code>STATIC_URL = 'static/'\n\n# add this\nSTATICFILES_DIRS = [\n    BASE_DIR / 'static',\n]\n</code></pre> </li> </ul> <p>Setting up static directory:</p> <ul> <li>In order to invoke the contents of this directory in the template htmls we need to do following:     <pre><code>{% extends \"base.html\" %}\n\n{% block content %}\n&lt;ul&gt;\n    {% for note in notes %}\n        &lt;li class=\"note-li\"&gt;{{note.title}}&lt;/li&gt;\n    {% endfor %}\n&lt;/ul&gt;\n{% endblock %}\n</code></pre></li> <li>Where the base html can be:     <pre><code>{% load static %}\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;link rel=\"stylesheet\" href=\"{% static 'css/style.css' %}\"&gt;\n    &lt;title&gt;Basecamp&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    {%block content%}\n    {%endblock%}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></li> <li>External CSS also can be used like Bootstrap or Tailwind</li> </ul> </li> </ul> CRUD - remaining parts <p>Until now we have dealt with the Read part of the CRUD operations. We will not deal with the CREATE, UPDATE AND DELETE parts</p> <p>CREATE:</p> <ul> <li>We will need to add a create end point</li> <li>Use the following code in views.py: <pre><code>from django.views.generic import CreateView, DetailView, ListView\n# Create your views here.\n\nclass NoteCreateView(CreateView):\n    model = Notes\n    fields = ['title', 'note']\n    success_url = '/smart/notes'\n</code></pre></li> <li>urls.py: <pre><code>from django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('notes', views.NoteListView.as_view(), name='notes.list'),\n    path('notes/&lt;int:pk&gt;', views.NoteDetailView.as_view(), name='notes.detail'),\n    path('notes/new' , views.NoteCreateView.as_view(), name='notes.new')\n]\n</code></pre></li> <li>Template html: <pre><code>&lt;!-- notes_form.html --&gt;\n{% extends \"base.html\" %}\n\n{%block content%}\n\n&lt;form action=\"{% url 'notes.new'%}\" method=\"POST\"&gt;{% csrf_token %}\n    {{form}}\n    &lt;button type=\"submit\" class=\"btn btn-primary my-5\"&gt;Submit&lt;/button&gt;\n&lt;/form&gt;\n{%endblock%}\n</code></pre></li> <li>Form validations are an important part and Django makes it easy to work with them</li> <li>Create a forms.py file in the app and add the code as illustrated below: <pre><code>from django import forms\nfrom django.core.exceptions import ValidationError\n\nfrom .models import Notes\n\nclass NotesForm(forms.ModelForm):\n    class Meta:\n        model = Notes\n        fields = ('title', 'note')\n        widgets = {\n            'title': forms.TextInput(attrs={'class' : 'form-control my-5'}),\n            'note': forms.Textarea(attrs={'class' : 'form-control mb5'})\n        }\n        labels = {\n            'note': \"What's on your mind?\"\n        }\n\n    def clean_title(self):\n        title = self.cleaned_data['title']\n        if 'Django' not in title:\n            raise ValidationError('Not a django related note')\n        return title\n</code></pre></li> <li>The above class needs to be simply included in the views.py in the create view section</li> <li>The above also adds ways to control UX elements like label and css injection using labels and widgets dictionary <pre><code>    class NoteCreateView(CreateView):\n    model = Notes\n    # fields = ['title', 'note']\n    success_url = '/smart/notes'\n    form_class = NotesForm\n</code></pre></li> <li>The form errors can be exluded by using css selectors and hiding them</li> <li>They can also be styled by adding the below code to the template: <pre><code>    {% if form.errors %}\n        &lt;div class=\"alert alert-danger my-5\"&gt;\n            {{form.errors.title.as_text}}\n        &lt;/div&gt;\n    {%endif%}\n</code></pre></li> </ul> <p>UPDATE</p> <ul> <li>Updating is really easy as it is a natural extension of the create part.</li> <li>First add the UpdateView in views.py:     <pre><code>from django.views.generic import CreateView, DetailView, ListView, UpdateView\n# Create your views here.\n\nclass NoteUpdateView(UpdateView):\n    model = Notes\n    # fields = ['title', 'note']\n    success_url = '/smart/notes'\n    form_class = NotesForm\n</code></pre></li> <li>Update the NoteUpdateView in urls.py which will add an edit end point     <pre><code>path('notes/&lt;int:pk&gt;/edit', views.NoteUpdateView.as_view(), name='notes.update'),\n</code></pre></li> <li>This will basically add the functionality update the fields.</li> <li>We can beautify it by adding relevant buttons to the templates</li> <li>For example and edit button on the detail page <code>&lt;a href=\"{% url 'notes.update' pk=note.id %}\" class=\"btn btn-secondary\"&gt;Edit&lt;/a&gt;</code></li> <li>And cancel button on the edit page: <code>&lt;a href=\"{% url 'notes.list' %}\" class=\"btn btn-secondary\"&gt;Cancel&lt;/a&gt;</code></li> </ul> <p>DELETE</p> <ul> <li>Deletion is quite similar to update.</li> <li>Create a DeleteView in views and include it in urls.py <pre><code>from django.views.generic.edit import DeleteView\nfrom .forms import NotesForm\n# Create your views here.\n\nclass NoteDeleteView(DeleteView):\n    model=Notes\n    success_url = '/smart/notes'\n    template_name = 'notes/notes_delete.html'\n</code></pre></li> <li>Inclusion for urls.py <pre><code>path('notes/&lt;int:pk&gt;/delete', views.NoteDeleteView.as_view(), name='notes.delete'),\n</code></pre></li> <li>Add a template to specify a confirmation message.</li> <li>Easy peasy. <pre><code>{%extends \"base.html\" %}\n\n{%block content%}\n&lt;form method=\"POST\"&gt;{% csrf_token %}\n    &lt;p&gt;Are you sure you want to delete \"{{notes.title}}\"?&lt;/p&gt;\n    &lt;input type=\"submit\" class=\"btn btn-danger\" value=\"confirm\"&gt;\n&lt;/form&gt;\n{%endblock%}\n</code></pre></li> </ul> User management <ul> <li>Django already has a user database with one user admin with pk=1</li> <li>We first need to create migrations and run them to associate all notes to the admin user</li> <li>This will also make the app user aware i.e. we should be able to show only those notes to users who created them</li> <li>First update the model: <pre><code>    from django.db import models\n    from django.contrib.auth.models import User\n\n    # Create your models here.\n    class Notes(models.Model):\n        title = models.CharField(max_length=200)\n        note = models.TextField()\n        created = models.DateTimeField(auto_now_add=True)\n        user = models.ForeignKey(User, on_delete=models.CASCADE, related_name=\"notes\")\n</code></pre></li> <li>Once done create migrations using <code>python manage.py makemigrations</code></li> <li>Then run the migration <code>python manage.py migrate</code></li> <li>It will ask for a default user. Enter 1 i.e. pk for admin</li> <li>This will associate all existing notes with the admin user</li> <li>Next update the views that need logins for example the list view: <pre><code>from django.contrib.auth.mixins import LoginRequiredMixin\n# Code before ---\nclass NoteListView(LoginRequiredMixin, ListView):\nmodel = Notes\ncontext_object_name = \"notes\"\ntemplate_name = 'notes/smart_notes.html'\nlogin_url = '/admin'\n\ndef get_queryset(self):\n    return self.request.user.notes.all()\n\n# Code after ---\n</code></pre></li> <li>Assigning a user to a new note:<ul> <li>To do this add the following method to the createview class: <pre><code>    def form_valid(self, form):\n        self.object = form.save(commit=False)\n        self.object.user = self.request.user\n        self.object.save()\n        return HttpResponseRedirect(self.get_success_url())\n</code></pre></li> <li>This will essentially stop the form from being saved, fetch the object and add a user to it.</li> </ul> </li> </ul> Login/Logout/Signup <ul> <li>Login/Logout/Singups are really easy too.</li> <li>Home/Views.py: <pre><code>    from django.contrib.auth.views import LoginView, LogoutView\n    from django.views.generic.edit import CreateView\n    from django.contrib.auth.forms import UserCreationForm\n    from django.shortcuts import redirect\n\n    class SignupView(CreateView):\n        form_class = UserCreationForm\n        template_name = 'home/register.html'\n        success_url = '/smart/notes'\n\n        def get(self, request, *args, **kwargs):\n            if self.request.user.is_authenticated:\n                return redirect('notes.list')\n            return super.get(request, *args, **kwargs)\n\n    class LogoutInterfaceView(LogoutView):\n        template_name = 'home/logout.html'\n\n    class LoginInterfaceView(LoginView):\n        template_name = 'home/login.html'\n</code></pre></li> <li>Home/Urls.py: <pre><code>    from django.urls import path\n    from . import views\n\n    urlpatterns = [\n        path('', views.HomeView.as_view(), name='home'),\n        path('login', views.LoginInterfaceView.as_view(), name='login'),\n        path('logout', views.LogoutInterfaceView.as_view(), name='logout'),\n        path('signup', views.SignupView.as_view(), name='signup'),\n    ]\n</code></pre></li> <li>Templates: <pre><code>&lt;!-- login --&gt;\n{%extends 'base.html'%}\n\n{%block content%}\n&lt;form method=\"POST\"&gt;{%csrf_token%}\n    {{form.as_p}}\n    &lt;input type=\"submit\" class=\"btn btn-secondary\"&gt;\n&lt;/form&gt;\n{%endblock%}\n\n&lt;!-- logout --&gt;\n\n{%extends 'base.html'%}\n\n{%block content%}\n&lt;h1&gt;Buh bye! \ud83d\udc4b&lt;/h1&gt;\n{%endblock%}\n\n&lt;!-- register --&gt;\n\n{%extends 'base.html'%}\n\n{%block content%}\n&lt;form method=\"POST\" style=\"text-align: left; margin: 0 auto; width: 600px;\"&gt;{%csrf_token%}\n    {{form.as_p}}\n    &lt;input type=\"submit\" class=\"btn btn-secondary\" name=\"Submit\"&gt;\n&lt;/form&gt;\n{%endblock%}\n</code></pre></li> </ul> Finishing touches <ul> <li>Add finishing touches by adding a navbar to the base.html <pre><code> &lt;nav class=\"navbar navbar-dark bg-dark\"&gt;\n    &lt;div class=\"ms-auto\"&gt;\n        {%if user.is_authenticated%}\n        &lt;a href=\"{% url 'home' %}\" class=\"btn btn-outline-light me-1\"&gt;Home&lt;/a&gt;\n        &lt;a href=\"{% url 'notes.list' %}\" class=\"btn btn-outline-light me-1\"&gt;Notes&lt;/a&gt;\n        &lt;a href=\"{% url 'notes.new' %}\" class=\"btn btn-outline-light me-1\"&gt;Create&lt;/a&gt;\n        &lt;a href=\"{% url 'logout' %}\" class=\"btn btn-outline-light me-1\"&gt;Logout&lt;/a&gt;\n        {%else%}\n        &lt;a href=\"{% url 'login' %}\" class=\"btn btn-outline-light me-1\"&gt;Login&lt;/a&gt;\n        &lt;a href=\"{% url 'signup' %}\" class=\"btn btn-outline-light me-1\"&gt;Signup&lt;/a&gt;\n        {%endif%}\n    &lt;/div&gt;\n&lt;/nav&gt;\n</code></pre></li> </ul> Connecting to Postgres <ul> <li>Your Postgres settings.py should look like this: <pre><code>    DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql',\n        'NAME': '&lt;database_name&gt;',\n        'USER': 'user_name',\n        'PASSWORD': 'password',\n        'HOST': 'localhost',\n        'PORT': '5430',\n        }\n    }\n</code></pre></li> <li>After this create migrations using <code>python manage.py makemigrations</code></li> <li>Next step is to run migrations <code>python manage.py migrate</code></li> <li>The data (models) should now show up in postgresql database</li> </ul> Example <ul> <li>The full coded example (notes application) can be found on this github repository</li> <li>To run this project just run <code>python manage.py runserver &lt;port&gt;</code></li> </ul>"},{"location":"Development/postgre/","title":"PostgreSQL Quick Start","text":"Installation <p>Two ways to install:</p> <ul> <li>OS specific installer<ul> <li>Just download the OS specific installation and complete recommended setup</li> </ul> </li> <li>Containerized approach using docker<ul> <li>Just run the below command in the terminal (docker needs to be installed and running): <pre><code>docker run --name postgre-server -e POSTGRES_PASSWORD=admin -p 5430:5432 -d postgres:latest\n</code></pre></li> </ul> </li> </ul> User Interfaces <p>PSQL Shell</p> <p>PSQL shell is a command line interface to work with the Postgre databases.</p> <p>Quick commands:</p> <ul> <li><code>\\l</code> list od databases</li> <li><code>\\c &lt;database name&gt;</code> change context to database</li> <li><code>SELECT version();</code> : shows the version</li> <li><code>SELECT now();</code>: shows server time</li> <li><code>CREATE DATABASE colors;</code> : create new database</li> <li><code>CREATE TABLE colors (ColorID int, ColorName char(20));</code> : creating a new table</li> <li><code>INSERT INTO colors VALUES (1, 'red') , (2, 'blue'), (3, 'green');</code>: inserting values</li> <li><code>SELECT * FROM colors;</code>: fetching data</li> </ul> <p>PGAdmin</p> <p>PGAdmin is a graphical user interface to connect to the PostgreSQL servers. Each server has 3 parts:</p> <ul> <li>Databases</li> <li>Login/Group roles</li> <li> <p>Tablespaces</p> </li> <li> <p>Tables are located in Databases-&gt;Db Name-&gt;Schema-&gt;Tables-&gt;Table Name</p> </li> <li>The client has a query tool to fetch data from the tables</li> <li>To quickly get all rows: From table right click menu select \"View/Edit Data-&gt; All rows\"</li> <li>To connect the PG Admin to postgres on docker just enter the login details and server details</li> </ul> Relational Databases <p>Database table structure:</p> <ul> <li>Each table has a rows and columns</li> <li>the first row is usually the primary key. convention: <code>tablename_id</code></li> <li>other columns might represent different types of data</li> </ul> <p>Common Datatypes:</p> <ul> <li>Numeric:<ul> <li>Whole numbers: <code>int``,</code>bigint<code>,</code>smallint` data types</li> <li>fractions: <code>numeric</code>, <code>decimal</code> data types. both will allow significant digit specification</li> <li>floating points: <code>real</code> or <code>double</code> precision</li> </ul> </li> <li>Characters<ul> <li>Use <code>character(n)</code> or <code>char(n)</code>: for fixed length </li> <li>Use <code>varchar(n)</code> for variable length</li> <li>Use <code>text</code> for unlimited text with no cap</li> </ul> </li> <li>Date &amp; Time<ul> <li><code>date</code></li> <li><code>time</code></li> <li><code>timestamp</code></li> <li><code>timestamp with timezones</code></li> </ul> </li> </ul> Sample database from scratch <p>For our sample we are going to consider a company called \"Kinetico\".  We will build the organizational structure in PostgreSQL using all of its main features.</p> <p>Step 1: Create Database</p> <p>In pgAdmin right click on databases-&gt;Create-&gt;database. This in turn will run the following command in the background: <pre><code>    CREATE DATABASE \"kinetico\"\n    WITH\n    OWNER = postgres\n    ENCODING = 'UTF8'\n    CONNECTION LIMIT = -1\n    IS_TEMPLATE = False;\n</code></pre></p> <p>Step 2: Schemas</p> <ul> <li>Schemas are more like the departments of an organization. </li> <li>It can be used to group together table logically.</li> <li>PostgreSQL has a default public schema under which all tables go if no other schema is created.</li> <li>To create a schema just: <code>Choose create -&gt;Schema</code> from the right click menu of schemas or databases.</li> <li>This will run the following command: <pre><code>    CREATE SCHEMA manufacturing\n    AUTHORIZATION postgres;\n</code></pre></li> <li>Create 2 schemas manufacturing and human_resources</li> </ul> <p>Step 3: Tables</p> <ul> <li> <p>For now we will add 2 tables to the manufacturing schema called products and categories with the following columns:</p> </li> <li> <p>products:</p> <ul> <li>product_id (primary_key)</li> <li>name</li> <li>power</li> <li>manufacturing_cost</li> <li>category_id</li> </ul> </li> <li> <p>categories:</p> <ul> <li>category_id</li> <li>name</li> <li>market</li> </ul> </li> </ul> <pre><code>CREATE TABLE manufacturing.products\n(\n    product_id character varying(10) NOT NULL,\n    name character varying(100) NOT NULL,\n    power integer,\n    manufacturing_cost numeric(10, 2) NOT NULL,\n    category_id integer NOT NULL,\n    PRIMARY KEY (product_id)\n);\n\nALTER TABLE IF EXISTS manufacturing.products\n    OWNER to postgres;\n</code></pre> <p>Step 4: Link tables with Foreign Keys</p> <ul> <li>In order to add link two tables we need to add a foreign key constraint to the table where the foreign key will be referenced.</li> <li> <p>For example in our case the the products table will reference the category id so we will apply the foreign key constraint to the product table.</p> </li> <li> <p>Using pgAdmin we can apply the Foreign Key constraint by going to table properties -&gt; constraints -&gt;foreign key</p> </li> <li>Here specify the tables and columns. Choose validated button and select cascade when updated.</li> </ul> <p>The sample SQL is as follows: <pre><code>ALTER TABLE IF EXISTS manufacturing.products DROP CONSTRAINT IF EXISTS products_category_id_fkey;\n\nALTER TABLE IF EXISTS manufacturing.products\n    ADD FOREIGN KEY (category_id)\n    REFERENCES manufacturing.categories (category_id) MATCH SIMPLE\n    ON UPDATE CASCADE\n    ON DELETE NO ACTION;\n</code></pre></p> <p>Step 5: Add data to tables with CSV</p> Querying Data <ul> <li> <ul> <li>SELECT : Allows to specify the columns needed to be query. If all needed use * as a shortcut</li> <li>FROM : Allows to specify the schema.table from where the query needs to be done</li> <li>WHERE : Allows to fetch only those rows which meet a criteria</li> </ul> <p>Basic clauses:</p> <pre><code>SELECT *\nFROM manufacturing.products;\n</code></pre> <pre><code>SELECT name, manufacturing_cost\nFROM manufacturing.products;\nWHERE product_id = 'KE9W';\n</code></pre> </li> <li> <p>Joining two tables to query data:</p> <ul> <li>SELECT: Use tablename.columnName comma separated values and use alias clause (AS) to specify ambiguous values</li> <li>FROM: Use <code>schemaName.tableName JOIN schemaName.tableName</code></li> <li>ON: Use to specify the categories on which joins will happen</li> <li>WHERE: Filter the data</li> </ul> <pre><code>SELECT products.product_id,\n    products.name AS product_name,\n    products.manufacturing_cost,\n    products.category_id,\n    categories.name AS category_name,\n    categories.market\nFROM manufacturing.products JOIN manufacturing.categories\nON products.category_id = categories.category_id\nWHERE categories.name = 'batteries';\n</code></pre> </li> <li> <p>Joining 3 tables:</p> <ul> <li>Joining 2 tables to 1 parent table where the 2 PKs are referenced as FKs <pre><code>SELECT bookauthors.id as baid,\n    books.book_name,\n    books.page_num,\n    authors.author_name,\n    authors.author_surname\nFROM bookauthors \nJOIN books ON bookauthors.book_id = books.book_id \nJOIN authors ON bookauthors.author_id = authors.author_id\nWHERE authors.author_name='Alex' AND authors.author_surname='Cross'\n</code></pre></li> </ul> </li> <li> <p>Creating a view to easily query joined data:</p> <ul> <li>In case the joined data needs to be queried multiple times then views can be created.</li> <li>Process is to write the query and add the following syntax before it: <code>CREATE VIEW tableName.viewName AS</code></li> <li>A view will be created which can be used a separate table</li> <li>A view acts like a table but does not duplicate data</li> </ul> </li> </ul> <pre><code>CREATE VIEW manufacturing.product_details AS\nSELECT products.product_id,\n    products.name AS product_name,\n    products.manufacturing_cost,\n    products.category_id,\n    categories.name AS category_name,\n    categories.market\nFROM manufacturing.products JOIN manufacturing.categories\nON products.category_id = categories.category_id;\n</code></pre> Indexing &amp; constraining data <ul> <li>Index is a typical like a collection of keywords you find at the end of a text book to quickly find concepts.</li> <li>You can use it to find information frequently needed by creating an index on a table.</li> <li>To create an index open the table needed and from the right click menu choose create</li> <li> <p>There are a few options in the creation panel like Unique, Clustered etc.</p> </li> <li> <p>Constraints are conditions we add to the database so that only the correct data is added</p> </li> <li>Click on Constraints-&gt;Create and add a name and condition(s) to create a constraint</li> </ul> Administration <ul> <li>Postgre provides some administrative capabilitites. </li> <li>We can use roles to provide unlimited or restricted access to the users.</li> <li>Some common syntax is mentioned below to:<ul> <li>Set role</li> <li>Reset role</li> <li>Grant permissions</li> <li>Revoke role</li> <li>drop the role</li> </ul> </li> </ul> <pre><code>-- View tables from the KinetEco database\nSELECT * FROM manufacturing.products;\nSELECT * FROM human_resources.employees;\n\n-- Impersonate the hr_manager\nSET ROLE hr_manager;\n\n-- Switch permissions back to posgres super user\nRESET ROLE;\n\n-- Give hr_manager permissions in database\nGRANT USAGE ON SCHEMA human_resources TO hr_manager;\nGRANT SELECT ON ALL TABLES IN SCHEMA human_resources TO hr_manager;\nGRANT ALL ON ALL TABLES IN SCHEMA human_resources TO hr_manager;\n\n-- Remove the hr_manager role from Postgres Server\nRESET ROLE;\nREVOKE ALL ON ALL TABLES IN SCHEMA human_resources FROM hr_manager;\nREVOKE USAGE ON SCHEMA human_resources FROM hr_manager;\nDROP ROLE hr_manager;\n</code></pre>"},{"location":"Development/python-database-api/","title":"Python Database API","text":"<p>Introduction</p> Info <ul> <li>There are multiple database systems out there. </li> <li>Even considering relational databases there are multiple implementations.</li> <li>Interacting with these databases can be tedius since the implementations are different.</li> <li>Python provides a neat standardized way to interact with these databases</li> <li>Its called the Python Database API</li> <li>It provides ways to connect to the database (connection API) and performing operations (cursor API) from python itself</li> </ul> <p>Creating a database in PostgreSQL</p> Info <ul> <li>Short answer? Go to pgAdmin and create a new database from the UI.</li> <li>For more details please refer to the PostgreSQL Quick start</li> </ul> <p>Creating a table in PostgreSQL using python</p> Info <ul> <li>For this we need a module called psycopg2</li> <li>The process is simple:<ul> <li>Open a connection to the database</li> <li>Open a cursor using the connection object</li> <li>Execute the command</li> <li>Commit the command</li> <li>Close the connection</li> </ul> </li> <li>Install the psycopg2 module using: <code>pip install psycopg2-binary</code></li> <li>Example code:     <pre><code>import psycopg2\n\nconn = psycopg2.connect(\n    database=\"red30\",\n    user=\"postgres\",\n    password=\"admin\",\n    host=\"localhost\",\n    port=\"5430\")\n\ncursor = conn.cursor()\n\ncursor.execute('''CREATE TABLE sales(\n    order_num INT PRIMARY KEY,\n    cust_name TEXT,\n    product_num TEXT,\n    product_name TEXT,\n    quantity INT,\n    price REAL,\n    discount REAL,\n    order_total REAL);''')\n\nconn.commit()\nconn.close()\n</code></pre></li> </ul> <p>Inserting data in PostgreSQL using Python</p> Info <ul> <li>The process remains the same.</li> <li>Open connection</li> <li>Get the cursor</li> <li>Execute commands:     <pre><code>    # open connection\n    # get cursor\n\n    sales = [(1,\"John Doe\",\"OBJ1\",\"ITE1\",2,89,0,178),\n    (2,\"Jane Doe\",\"OBJ2\",\"ITE3\",1,44.95,0,44.95),\n    (3,\"John Doekar\",\"OBJ7\",\"ITE7\",3,399,0.1,1077.3)]\n\n    cursor.executemany(\"INSERT INTO sales VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", sales)\n\n    # commit changes\n    # close connection\n</code></pre></li> </ul> <p>Interacting with PostgreSQL database using Python</p> Info <ul> <li>A typical program to insert values based on some logic running in python is shown below.</li> <li>Here the function insert_sale takes input taken as arguments, calculates order total and inserts values: <pre><code>def insert_sale(order_num, customer_name, product_num, product_name, quantity, price, discount, order_total):\norder_total = quantity * price\nif discount!=0:\n    order_total -= discount\n#create a dictionary\nsale_data = {\n    'order_num': order_num,\n    'cust_name': customer_name,\n    'product_num': product_num,\n    'product_name': product_name,\n    'quantity': quantity,\n    'price': price,\n    'discount': discount,\n    'order_total': order_total}\n\ncursor.execute(\"INSERT INTO sales VALUES(%(order_num)s, %(cust_name)s, %(product_num)s, %(product_name)s, %(quantity)s, %(price)s, %(discount)s, %(order_total)s)\", sale_data)\n</code></pre></li> </ul> <p>SQLAlchemy Core to connect  manipulate to a Postgres database</p> Info <ul> <li>Install SQL Alchemy core using <code>pip install sqlalchemy</code></li> <li>To connect to a database table use the below code: <pre><code>from sqlalchemy import create_engine, select\nfrom sqlalchemy import Table, Column, Integer, Float, String, MetaData\n\n# Create the engine object using create_engine function\nengine = create_engine('postgresql+psycopg2://postgres:admin@localhost:5430/red30', \necho=True)\n\n# Create the metadata object\nmetadata = MetaData()\n\n# Build the table structure as a list if need to create using Table object\n\n# sales_table = Table('sales', \n#       metadata,  \n#       Column('order_num', Integer, primary_key=true),\n#       Column('cust_name', String),\n#       Column('prod_number', String),\n#       Column('prod_name', String),\n#       Column('quantity', Float),\n#       Column('price', Float),\n#       Column('discount', Float),\n#       Column('order_total', Float))\n\n# Connect to the table with the Table object\n\nsales_table = Table('sales', metadata, autoload_with=engine)\n\n# pass engine object to the metadata create_all function\nmetadata.create_all(engine)\n\nwith engine.connect() as conn:\n    #Read\nfor row in conn.execute(select(sales_table)):\n    print(row)\n\n    # Create\n    insert_statement = sales_table.insert().values(order_num=1105910, \n        cust_name='Syman Mapstone', \n        product_num='EB521', \n        product_name='Understanding Artificial Intelligence', \n        quantity=3,\n        price=19.5, \n        discount=0, \n        order_total=58.5)\n    conn.execute(insert_statement)\n\n    # Update\n    update_statement = sales_table.update().where(sales_table.c.order_num==1105910).values(quantity=2, order_total=39)\n    conn.execute(update_statement)\n\n    # Confirm Update\n    reselect_statement = sales_table.select().where(sales_table.c.order_num==1105910)\n    updated_sale = conn.execute(reselect_statement).first()\n    print(updated_sale)\n\n    # Delete\n    delete_statement = sales_table.delete().where(sales_table.c.order_num==1105910)\n    conn.execute(delete_statement)\n\n    # Confirm Delete\n    not_found_set = conn.execute(reselect_statement)\n    print(not_found_set.rowcount)\n</code></pre></li> </ul> <p>SQLAlchemy ORM to connect  manipulate to a Postgres database</p> Info <ul> <li>Core is a function based approach to connect while ORM is class based approach.</li> <li>Sample crud operations along with connections etc are shown below:     <pre><code>from sqlalchemy import create_engine, select\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.ext.automap import automap_base\n\nengine = create_engine('postgresql://postgres:password@localhost/red30')\n\n# class Sale(Base):\n#       __tablename__='sales',\n#       Column('order_num', Integer, primary_key=true),\n#       Column('cust_name', String),\n#       Column('prod_number', String),\n#       Column('prod_name', String),\n#       Column('quantity', Float),\n#       Column('price', Float),\n#       Column('discount', Float),\n#       Column('order_total', Float))\n\nBase = automap_base()\n\nBase.prepare(autoload_with=engine)\n\nSales = Base.classes.sales\n\nwith Session(engine) as session:\n\n    # Read\n    smallest_sale = session.execute(select(Sales).order_by(Sales.order_total)).scalar()\n    print(smallest_sale.order_total)\n\n    # Insert\n    recent_sale = Sales(order_num=1105910, cust_name='Syman Mapstone', prod_number='EB521', prod_name='Understanding Artificial Intelligence', quantity=3, price=19.5, discount=0, order_total=58.5)\n    session.add(recent_sale)\n    session.commit()\n\n    # Update\n    recent_sale.quantity = 2\n    recent_sale.order_total = 39\n    updated_sale = session.execute(select(Sales).filter(Sales.order_num == 1105910)).scalar()\n    print(updated_sale.quantity)\n    print(updated_sale.order_total)\n    session.commit()\n\n    # Delete\n    returned_sale = session.execute(select(Sales).filter(Sales.order_num == 1105910)).scalar()\n    session.delete(returned_sale)\n    session.commit()\n</code></pre></li> </ul>","tags":["python"]},{"location":"Development/sql/","title":"SQL - Complete overview","text":""},{"location":"Development/sql/#basics","title":"Basics","text":"<p>Overview of basic SQL commands</p> Info <ul> <li>The structured query language or SQL comprises of 5 sub languages <pre><code>graph TB\nA[SQL - Structured Query Language] --&gt; B[DDL - Data definition language]\nA --&gt; C[DML - Data manipulation language]\nA --&gt; D[DCL - Data control language]\nA --&gt; E[TCL - Transaction control language]\nA --&gt; F[DQL - Data Query language]</code></pre></li> <li> <p>DDL: Data definition language</p> Command Description CREATE Creates new db or table ALTER Modifies the structure of db or table TRUNCATE Removes all table records and allocated table spaces DROP Deletes a db or table RENAME Renames a table or db </li> <li> <p>DML: Data Manipulation language</p> <ul> <li>When you need to change the data itself or perform operations you use DML</li> </ul> Command Description INSERT Adds new row to a table UPDATE Updates the existing rows DELETE Deletes records from a table MERGE Also called UPSERT add or update data based on conditions </li> <li> <p>DCL - Data Control language</p> <ul> <li>Control = Authorization.</li> <li>For authorization you either grant or revoke access</li> </ul> Command Description GRANT Gives access previliges to user REVOKE Removes access previliges </li> <li> <p>TCL - Transaction control language</p> <ul> <li>Data manipulation happens outside of the container and added to container via transactions</li> </ul> Command Description COMMIT Saves changes to database permanently ROLLBACK Restores the database to original form until last commit SAVEPOINT Creates point for later use for rollback SET TRANSACTION Set transaction properties to make it read-only </li> <li> <p>DQL - Data Query language</p> <ul> <li>Single keyword in this criteria: <code>SELECT</code></li> <li>Retrieves information from the database based on parameters</li> </ul> </li> <li> <p>Data Types</p> <ul> <li>Typical data types are fo 5 main types:<ul> <li>Numeric: INT, BIGINT, FLOAT, REAL</li> <li>String: CHAR, VARCHAR</li> <li>Binary: BINARY, VARBINARY</li> <li>Miscellaneous: DATE, TIME, DATETIME</li> <li>Proprietary: Unique to the DB system. For example MSSQL has MONEY as a type</li> </ul> </li> </ul> </li> <li> <p>Basic Queries:</p> <ul> <li>To insert values in a table:     <pre><code>INSERT INTO public.product(\n    productid, categoryid, supplierid, productname, netretailprice, availableqty, wholesaleprice, unitkgweight, notes)\n    VALUES (1, 5, 2, 'Calculator' ,24.99 ,100 ,17.99 ,1, 'app'),\n    (2, 5, 5, 'Penwrite' ,79.99 , 27, 49.99, 2, 'device'),\n    (3, 1, 6, 'Vortex generator' ,2499.99 , 1000, 1999.99, 0.01, 'space engine'),\n    (4, 1, 6, 'Gourmet crockpot' ,24.99 , 72, 19.99, 1.63, 'utensil');\n</code></pre></li> <li>To get the data from a table for all columns: <code>SELECT * FROM public.product;</code></li> </ul> </li> </ul> <p>Manipulating Data</p> Info <ul> <li>The INSERT operation:<ul> <li>The command inserts one or more rows in a table</li> <li>Ground rules:<ul> <li>There is no need to provide all the values for every INSERT operation unless there is a constraint</li> <li>Some columns have defualt or auto generated values. </li> <li>Auto generated values should not be altered</li> <li>Column values must always match the sequence, data-type and size requirements</li> <li>Numbers should not be provided in quotes. Strings, characters and date-time must be provided in quotes</li> <li>If names of columns are not provided then the values must be provided in a strict sequence</li> <li>Insert can happen only on 1 table at a time</li> </ul> </li> <li>Tips:<ul> <li>While creating a table in PostgreSQL we can use the <code>SERIAL</code> data type to auto increment IDs</li> <li>For date we can use NOW() function in the default values tab to auto add the creation date</li> <li>In case the column name is provided but we need provide the default value we can use the the <code>DEFAULT</code> keyword in the values clause <pre><code>INSERT INTO department (\n\u00a0\u00a0\u00a0\u00a0departmentName,\n\u00a0\u00a0\u00a0\u00a0departmentLoc)\nVALUES\n(\n\u00a0\u00a0\u00a0\u00a0'Administration',\n\u00a0\u00a0\u00a0\u00a0DEFAULT\n),\n(\n\u00a0\u00a0\u00a0\u00a0'IT',\n\u00a0\u00a0\u00a0\u00a0DEFAULT\n);\n</code></pre></li> </ul> </li> <li>We can create new table which has the some or all the values of another table:     <pre><code>    CREATE TABLE deptdemo AS\n    SELECT * FROM departments;\n</code></pre></li> </ul> </li> <li> <p>The DELETE Operation:     <pre><code>DELETE FROM employees\nWHERE empno = 1234;\n</code></pre></p> </li> <li> <p>The ALTER operation:</p> <ul> <li>ALTER allows to change one or more properties of a table.</li> <li>It allows making changes to the schemas in the database</li> <li>Example changing the name of a column: <pre><code>ALTER TABLE departments RENAME COLUMN dept_name to deptname;\n</code></pre></li> </ul> </li> <li>The UPDATE operation:<ul> <li>The operation consists of the SET operation and a WHERE CLAUSE</li> <li>the SET clause defines what to do</li> <li>The where clause defines what filters to apply: <pre><code>UPDATE product\nSET\n    netretailprice = netretailprice * 0.90\nWHERE\n    netretailprice = 24.99;\n</code></pre></li> </ul> </li> </ul> <p>Querying Data</p> Info <ul> <li>The SELECT statement:<ul> <li>A typical SQL statement is as follows: <pre><code>SELECT [COLUMNS LIST] \nFROM [TABLE NAME]\nWHERE [CONDITION]\nORDER BY [COLUMN NAME] [ASC|DESC] \nLIMIT [N]\n</code></pre></li> </ul> </li> <li>The column list can be comma separated like <code>product_name, quantity</code></li> <li>If we want to fetch all columns then use <code>*</code></li> <li>The FROM clause can also be provided as a comma separated list but atleast one is required</li> <li>The WHERE clause provides a way to put conditions on the query</li> <li>The ORDER BY clause provides a way to arrange the output in a specific order</li> <li>LIMIT allows to set a limit on the output</li> <li>Example Query:     <pre><code>SELECT id, product_id,total, quantity \nFROM orders\nWHERE quantity &gt; 5\nORDER BY total desc\nLIMIT 5;\n</code></pre></li> <li> <p>The WHERE clause:</p> <ul> <li>The WHERE clause is used to shape data as per conditions</li> <li> <ul> <li>Comparison:</li> </ul> <p>There are various conditions that can be used:</p> Operation Sign Finding values between WHERE  Between val1 AND val2 Boolean Operation &gt;, &lt; , &lt;=, &gt;=, !=, &lt;&gt; <ul> <li> <ul> <li>Can be used to specify character patterns</li> <li>Usage: <code>WHERE &lt;column name&gt; LIKE 'pattern'</code></li> <li>Example <code>WHERE product_name LIKE '_p%'</code> - This will find all products with name having 2<sup>nd</sup> letter as p</li> <li>Pattern table for quick reference:</li> </ul> <p>LIKE clause</p> Condition Pattern Any value that ends with letter a '%a' Any value that starts with letter a 'a%' Any value that has letter a '%a%' Any value that starts with b and ends with letter a 'b%a' Any value that has a as a second letter '_a%' Any value starts with a and has atleast 3 characters 'a_%_%' </li> </ul> <li> <p>The JOIN clause</p> <ul> <li>Used to fetch data from 2 or more tables together</li> </ul> Type Description Example Usage INNER Used to find data common to both tables Finding who ordered from the store <code>FROM ORDERS JOIN PEOPLE</code> RIGHT Used to find data from 2<sup>nd</sup> table and matching data in first Finding all people who have or have not ordered from the store <code>FROM ORDERS RIGHT JOIN PEOPLE</code> LEFT Used to find data from 1<sup>st</sup> table and matching data in second Finding all products which may or may not have been ordered <code>FROM PRODUCTS LEFT JOIN PEOPLE</code> CROSS Combine data from one column of table 1 and another column of table 2 into 1 column Find all facecard and value cards <code>FROM FACECARD CROSS JOIN VALUECARDS</code> UNION Used to combine 2 queries <code>&lt;Query 1&gt; UNION &lt;Query 2&gt;</code> </li> <li> <p>The CASE clause</p> <ul> <li>The case clause allows to add conditional values in a variety of operations e.g. SELECT</li> <li>Typical example, changing the column values to a specific string based on a condition <pre><code>SELECT products.cost,\nCASE\n    WHEN cost &lt; 10 'cheap'\n    WHEN cost &gt; 10 AND cost &lt; 50 'medium'\n    ELSE 'expensive'\nEND\nFROM manufacturing.products\n</code></pre></li> </ul> </li> <li>Managing repeat complex queries with VIEW<ul> <li>Before any select query just add <code>CREATE VIEW &lt;VIEW_NAME&gt; AS ...</code> <pre><code>CREATE VIEW high_value_orders\nAS\nSELECT * from orders\nWHERE ORDERS.price &gt; 30\n</code></pre></li> <li>The views can be treated as another table</li> </ul> </li> <p>Aggregate Functions</p> Info <ul> <li>The aggregate functions are as follows:<ul> <li>SUM</li> <li>AVG</li> <li>COUNT</li> <li>MIN</li> <li>MAX </li> </ul> </li> <li>The aggregate functions can be complimented by the GROUP_BY clause to provide some breakouts</li> <li>Example: <pre><code>SELECT    PC.ProductCategoryID, PC.ProductCategoryName, AVG(P.UnitKGWeight) AS 'AVERAGE PRODUCT KG WEIGHT', \n          MIN(P.NetRetailPrice) AS 'MINIMUM NET RETAIL PRICE'\nFROM    ProductCategories PC INNER JOIN\n        Products P ON PC.ProductCategoryID = P.ProductCategoryID\nGROUP BY  PC.ProductCategoryName, PC.ProductCategoryID;\n</code></pre></li> <li>In order to filter results based on an aggregation we need to use the HAVING clause. The where clause does not help <pre><code>SELECT  PC.ProductCategoryName, SUM(P.AvailableQuantity) AS 'TOTAL COUNT OF ALL PRODUCTS IN PRODUCT CATEGORY'\nFROM    Products P INNER JOIN ProductCategories PC ON\n        P.ProductCategoryID = PC.ProductCategoryID\nGROUP BY  PC.ProductCategoryName\nHAVING    SUM(P.AvailableQuantity) &gt; 250\nORDER BY  ProductCategoryName;\n</code></pre></li> </ul>"},{"location":"Management/","title":"Management","text":"<p>All the knowledge related to management topics.</p>"},{"location":"Management/#scrum","title":"Scrum","text":"<p>Complete knowledge on the Scrum framework</p> <p>Explore \u2197</p>"},{"location":"Management/#safe","title":"SAFE","text":"<p>Complete knowledge on the SAFE framework</p> <p>Explore \u2197</p>"},{"location":"Management/kanban/","title":"Kanban","text":""},{"location":"Management/kanban/#understanding-lean-principles","title":"Understanding Lean Principles","text":"<ul> <li>Kanban  Lean principles have their roots in Japanese culture  Automobile manufacturing</li> <li>These were adapted and moulded to work with Product development in general</li> <li>Kanban means \"Signal Cards\"</li> <li>Its a pull system where the customers will pull the product when needed and workers will create products in appropriate batches and keep the ball rolling</li> <li>It prioritizes use of small batches to improve the flow of the work rather than planning and scheduling</li> <li>The Kanban board is a signalling device</li> </ul> <p>Five principles of Kanban: - Visualize workflow - Limit WIP - Measure and monitor flow - Make policies explicit - Find opportunities for improvement</p>"},{"location":"Management/kanban/#start-enterprise-lean","title":"Start Enterprise Lean","text":"<ul> <li>Value Stream Mapping:<ul> <li>The idea of mapping the whole process and identifying the tasks that add value and rejecting waste</li> <li>Waste: All the things you do that do not add value to the customer</li> <li>While creating a value stream map lay out the entire process</li> <li>Identify areas that can be removed to elimninate waste</li> </ul> </li> <li>Create a pull system:<ul> <li>Create a board which will have typically 5 columns:<ul> <li>Input</li> <li>Analyze</li> <li>Develop</li> <li>Test</li> <li>Release</li> </ul> </li> <li>Add cards on the board and allow people to pull the cards as per availability</li> </ul> </li> <li>Determine the demand on the team:<ul> <li>Understand what is the current demand</li> <li>Items that are being delivered</li> <li>Based on current demand we can limit WIP</li> </ul> </li> <li>Determine Capacity:</li> </ul>"},{"location":"Management/pmbok/","title":"Project Management - Body of knowledge [PMBOK]","text":"<p>Notes on the principles of PMBOK. SKills essential for managers. PMBOK is a guide for project management, published by PMI. It has 12 principles and eight domains for delivering project outcomes. It is not a fixed method, but a flexible framework that can be customized and combined with other methods. It helps project managers and teams communicate and work better.</p>"},{"location":"Management/pmbok/#standard-for-project-management","title":"Standard for Project Management","text":""},{"location":"Management/pmbok/#introduction","title":"Introduction","text":"<ul> <li>The standard acts as a guide for the actions and behaviours needed to be taken project professionals at appropriate times.</li> <li>It provides a basis for understanding PM and how it enables intended outcomes</li> <li>is industry,location,size, delivery method agnostic</li> <li>describes project operations, governance, functions</li> <li>defines relationship b/w project and product management</li> </ul>"},{"location":"Management/pmbok/#key-terms","title":"Key terms:","text":"<ul> <li>Outcomes : Artifacts, outputs along with value and benefits</li> <li>Portfolio : Related projects, programs, operations managed as a single entity</li> <li>Product: Quantifyable component or artifact that is an end item</li> <li>Program: A collection of related projects that are managed as an entity that provides benefits not available by managing them together</li> <li>Project: A temporary endevour to provide a product, service or result</li> <li>Project Management: Guiding projects to intended outcomes using knowledge, skills, tools, techniques and approaches (predictive, hybrid, adaptive)</li> <li>Project Manager: facilitates project team work and processes</li> <li>Project Team: </li> <li>System for value delivery: Strategic business activities aimed at taking the org forward</li> <li>Value: Value, worth or usefulness. percieved value is different for different stakeholders</li> </ul>"},{"location":"Management/pmbok/#value-delivery-systems","title":"Value Delivery Systems","text":""},{"location":"Management/pmbok/#creating-value","title":"Creating Value","text":"Info <ul> <li> <p>Projects create value for organizations by:</p> <ul> <li>creating a product, service or desired result for end customers</li> <li>societal or environmental positive change</li> <li>improving efficiency, productivity, effectiveness or responsiveness</li> <li>enabling change to help transition to future state</li> <li>sustaining changes from previous programs</li> </ul> </li> <li> <p>Components of value delivery:</p> <ul> <li>Portfolios, projects, programs, products and operations are components of value delivery</li> <li>Working together these components deliver value in line with org strategy</li> <li>Both programs and projects can be standalone</li> <li>Ops can influence and support all the components aling with other functions like payroll</li> <li>Portfolios, programs, projects influence each other as well as ops</li> <li>Components create outcomes (end results) which create benefits which is a net gain for an organization</li> <li>Focus on outcomes, choices, decisions directly proportional to long range performance </li> </ul> </li> <li> <p>The components can be visualized as follows:     </p> </li> <li> <p>Information Flow </p> </li> </ul>"},{"location":"Management/pmbok/#organizational-governance-systems","title":"Organizational Governance Systems","text":"Info <ul> <li>Governance systems work along side value delivery systems</li> <li>They enable smooth workflow, resolve issues and support decision making</li> <li>they provide a framework with functions and processes that guide activities</li> <li>they include elements of oversight, control, value assessment, component integration and decision making</li> <li>provide structure to assess changes, risks and issues</li> </ul>"},{"location":"Management/pmbok/#functions-of-a-project","title":"Functions of a project","text":"Info <ul> <li> <p>Introduction:</p> <ul> <li>People drive project delivery</li> <li>Two main modes of co-ordiation: Centralized (central authority) and decentralized (self-managed)</li> <li>Sometimes hybrid mode of co-ordination also works (central authority with self-managed modules)</li> <li>In either system supportive leadership models, meaningful engagement with stakeholders underpin outcomes</li> <li>The functions in a team will vary depending on lot of factors like deliverables, industry, organization etc.</li> </ul> </li> <li> <p>Functions:</p> <ul> <li>Oversight and co-ordination<ul> <li>Planning, monitoring and control activities</li> <li>Co-ordinating: Collaborating with executive and business leaders to expand on objectives</li> <li>oversight: follow-on activites such as benefits realization and sustainment</li> <li>Fits well with Portfolio and Program functions</li> </ul> </li> <li>Present objectives and feedback<ul> <li>provide insights, perspectives and clear directives from customers and end-users</li> </ul> </li> <li>Facilitation and Support<ul> <li>encourage team member participation, collaboration, shared sense of responsibility </li> <li>create consensus, resolve conflicts and make decisions</li> </ul> </li> <li>Perform taks and provide insights<ul> <li>people that do the work and produce outcomes and artifacts</li> </ul> </li> <li>Apply expertise<ul> <li>provide knowledge vision and expertise in a specific area</li> </ul> </li> <li>Provide business expertise and direction<ul> <li>prioritizing requirements or backlog based on business value</li> <li>interacting with stakeholders, customers and teams to decide product direction</li> </ul> </li> <li>Provide resources<ul> <li>promote the project and secure resources</li> </ul> </li> <li>Maintain Governance<ul> <li>link between project and strategy teams</li> <li>approve recommendations by teams</li> <li>monitor progress</li> </ul> </li> </ul> </li> </ul>"},{"location":"Management/pmbok/#the-project-environment","title":"The Project Environment","text":"Info <ul> <li>Internal:<ul> <li>Process assets</li> <li>Governance docs</li> <li>Data assets</li> <li>Knowledge assets</li> <li>Security</li> <li>Org culture and structure</li> <li>Staff capability</li> <li>Geographic distribution</li> <li>IT software</li> <li>Infrastructure</li> <li>Resource availability</li> </ul> </li> <li>External:<ul> <li>Marketplace conditions</li> <li>Societal influence</li> <li>Regulatory</li> <li>Commercial databases</li> <li>Academic research</li> <li>Industry standards</li> <li>Financial considerations</li> <li>Physical considerations</li> </ul> </li> </ul>"},{"location":"Management/pmbok/#product-management-considerations","title":"Product Management considerations","text":"Info <ul> <li>Product Management involves integrating data, processes, people and business systems to create, maintain &amp; develop a product or service throughout its lifecycle.</li> <li>Forms:<ul> <li>Program management within a product lifecycle</li> <li>Project management within a product lifecyle</li> <li>Product managment within a program </li> </ul> </li> </ul>"},{"location":"Management/pmbok/#project-management-principles","title":"Project Management principles","text":"<ul> <li>The principles are not prescriptive</li> <li>They are broad based to allow multiple individuals and orgs can align to them</li> <li>They are internally consistent (one principle does not negate the other)</li> </ul>"},{"location":"Management/pmbok/#stewardship","title":"Stewardship","text":"Info <ul> <li>It means taking care, responsible use of resources and upholding values and ethics</li> <li>Two main considerations:<ul> <li>Internal:<ul> <li>Operating in alignment with orgs values</li> <li>Commitment towards and respectful engagement of team mates</li> <li>Responsible use of orgs resources like finance</li> <li>Appropriate use of authority, accountability and responsibility</li> </ul> </li> <li>External:<ul> <li>Environment Sustainability</li> <li>Orgs relationship with external stakeholders</li> <li>Societal impact</li> <li>Imapct on the industry practices</li> </ul> </li> </ul> </li> <li>Stewards have both implicit and explicit duties that may include:<ul> <li>Compliance</li> <li>Integrity</li> <li>Care</li> <li>Trustworthiness</li> </ul> </li> </ul>"},{"location":"Management/pmbok/#collaboration","title":"Collaboration","text":"Info <ul> <li>Collaboration includes multiple contributing factors like:<ul> <li>Agreements<ul> <li>these are behavioural patterns and working norms</li> <li>they need to be established at the start of the project</li> <li>evolved further to ensure collective working continues</li> </ul> </li> <li>Organizational structures<ul> <li>Structures that can improve collaboration include:<ul> <li>Definitions of roles and responsibilities</li> <li>Allocation of employees and vendors into project teams</li> <li>formal committees tasked with specific objectives</li> <li>Stand up meetings to review topics</li> </ul> </li> </ul> </li> <li>Processes<ul> <li>set of rules that enable completion of tasks and assignments</li> <li>these may include a work breakdown structure, taskboard or backlog</li> </ul> </li> </ul> </li> <li>By fostering inclusive and collaborative environments, knowledge and expertise are more freely exchanged, which in turn enable better project outcomes.</li> <li>Clearly defined roles can improve project outcomes significantly</li> <li>To clearly define roles the following need to be considered:<ul> <li>Authority: Taking decisions</li> <li>Accountability: Owning outcomes</li> <li>Responsibility: obligation for fulfilment</li> </ul> </li> <li>A collaborative project team takes collective ownership of all outcomes</li> </ul>"},{"location":"Management/pmbok/#engage-with-stakeholders","title":"Engage with stakeholders","text":"Info <ul> <li>Stakeholders are parties that affect, are affected by or percieve to be affected by a decision or deliverable of a program, project or portfolio</li> <li>They influence the project, its performance or outcome (both positive or negative)</li> <li>They affect:<ul> <li>Scope / Requirements</li> <li>Schedule</li> <li>Cost</li> <li>Plans</li> <li>Project team</li> <li>Outcomes</li> <li>Benefits realization</li> <li>Risk</li> <li>Quality</li> <li>Sucess</li> <li>Culture</li> </ul> </li> <li>Stakeholders will have a varying degree of interest  influence in the project</li> <li>The engagement level, methodology should be determined based on these factors</li> <li>For example low interest, low influence stakeholders should be engaged in a monitoring capacity and the written communication must be used</li> </ul>"},{"location":"Management/pmbok/#focus-on-value","title":"Focus on Value","text":""},{"location":"Management/pmp-study/","title":"PMI - PMP Study guide","text":"","tags":["Project-management"]},{"location":"Management/pmp-study/#section-1-project-management-people","title":"Section 1 - Project Management &amp; People","text":"<p>Introduction</p> Info <ul> <li> <p>What are process groups?</p> <ul> <li>There are 5 process groups:<ul> <li>Initiation</li> <li>Planning</li> <li>Execution</li> <li>Monitoring and control</li> <li>Closure</li> </ul> </li> <li>Process groups are not to be confused with a lifecycle</li> <li>They are containers of individual processes &amp; distinct ways of managing projects based on knowledge necessary</li> <li>Important point to note is projects are \"temporary\" &amp; \"unique\". These are their main characteristics</li> </ul> </li> <li> <p>Knowledge areas overview:</p> <ul> <li>Knowledge areas are specific concepts that are needed to be understood for each project</li> <li>They can be different for different projects</li> <li>But a Project Manager needs to understand which knowledge areas are needed for their work</li> <li>For example some projects might only need scope, schedule and cost</li> <li>There are a total of 10 knowledge areas which have processes, input, tools, techniques and outputs</li> <li>The knowledge areas might have processes spread accross multiple process groups</li> <li>A project manager must consider each of these knowledge areas before initiation to determine which one of these need to be planned, executed, monitored and controlled</li> </ul> </li> <li> <p>Knowledge areas:</p> <ul> <li>1 Integration management<ul> <li>It is the area where a comprehensive project management plan is created</li> <li>It involves creating a charter, deliverables, deciding where the monitoring and control happens, where formal change control lies and where closure occurs</li> <li>Integrate: All the chosen knowledge areas must integrate correctly </li> </ul> </li> <li>2 Scope management<ul> <li>Co-ordination of requirement collection for both project and product, determining how the scope of work and how the work will be completed and formal deliverable sign-off</li> <li>It is about ensuring the work is completed as expected by the stakeholders</li> <li>It is the act of ensuring you have the right tools, techniques and resources to complete the deliverable</li> <li>Scope of work is critical and it influences schedules, budgets and resource assignments</li> </ul> </li> <li>3 Schedule management<ul> <li>Answers the question: \"How long will the project take?\"</li> <li>Involves:<ul> <li>creating a sequence</li> <li>determining the resources needed to complete the task in a sequence and within assumed duration</li> <li>Produce a schedule that we can meet</li> </ul> </li> </ul> </li> <li>4 Cost management<ul> <li>Estimating costs</li> <li>Budgeting costs</li> <li>Controlling costs</li> </ul> </li> <li>5 Quality management<ul> <li>Its about building the thing right and fit for use</li> <li>Scope management is about building the right thing</li> </ul> </li> <li>6 Resource management<ul> <li>Managing people, equipment and material</li> <li>Involves team management and allocation of resources</li> </ul> </li> <li>7 Communication management<ul> <li>Involves creating a communication plan</li> <li>To get the right information to the right people at the right time and in the right format</li> <li>The communication needs to be planned to get the maximum impact</li> </ul> </li> <li>8 Risk management<ul> <li>This is about protecting project work, budgets, schedules etc.</li> <li>Two main types: Threats &amp; opportunities</li> <li>The area deals with analyzing the risks and creating a response plan</li> </ul> </li> <li>9 Procurement management<ul> <li>Contracts and aggreements with vendors</li> </ul> </li> <li>10 Stakeholder management<ul> <li>Stakeholders are the ones who have significant interest or influence on the project</li> <li>Their expectations need to be managed</li> <li>Effective communication is key to achieve this</li> </ul> </li> <li>Stakeholder engagement and Communication management work hand in hand but we need to think of it functionally.</li> <li>Communication is about information and getting it to the right people</li> <li>Stakeholder engagement is about relationships</li> </ul> </li> <li> <p>Processes:</p> <ul> <li>There are a total of 49 processes</li> <li>Each process correspond to one or more knowledge area &amp; one or more process groups</li> <li>Each process has a ITTO: input, tools, techniques &amp; output</li> </ul> </li> <li> <p>What are ITTOs?</p> <ul> <li>They answer the following questions:<ul> <li>What do I need before I can make X? \u2192 Inputs</li> <li>What will I use to do X? \u2192 Tools &amp; Techniques</li> <li>What will I have when I am finished? \u2192 outputs</li> </ul> </li> </ul> </li> <li> <p>OPAs (Organizational process assets) and EEFs (Enterprise environmental factors):</p> <ul> <li>These are the most common inputs and updated outputs in the processes also called influences</li> <li>It refers to process and environment i.e. what your organization does and who your organization is</li> <li>The consideration for your current org process and environment must be considered for any process</li> <li>\"Anything absolute in project management is regulation, not standards, or best practice\" </li> <li>EEFs:<ul> <li>Internal<ul> <li>Organizational culture, structure, and governance</li> <li>Geographic location and distribution of facilities and resources</li> <li>Infrastructure</li> <li>Information technology software</li> <li>Availability of resources</li> <li>Employee capability, skills, and specialized knowledge</li> </ul> </li> <li>External<ul> <li>Marketplace conditions</li> <li>Social and cultural influences and issues</li> <li>Legal restrictions</li> <li>Commercial databases</li> <li>Industry studies, publications, and benchmarking results</li> <li>Government or industry standards/regulations</li> <li>Financial considerations for inflation rates, exchange rates, tariffs, and geographic locations</li> <li>Physical environmental elements such as weather and working conditions</li> </ul> </li> </ul> </li> <li>OPAs:<ul> <li>Plans, policies, or knowledge of all performing organizations (yours or your customer's)</li> <li>Processes, policies, and procedures</li> <li>Organizational knowledge bases</li> <li>All can be established by the Project Management Office (PMO) or some other driving process from outside of your organization.</li> </ul> </li> </ul> </li> <li> <p>Project management plan &amp; project documents:</p> <ul> <li>There are numerous documents that a project manager must handle</li> <li>These need to be updated via change control or regular iterative updates</li> <li>There are certain rules based on the type of documents</li> <li>Input / Output rules:<ul> <li>Inputs are documents that are key to the process</li> <li>Outputs can be inputs to another process unless they are terminal or embedded into other inputs</li> <li>Inputs should be an output of a process unless its external</li> </ul> </li> <li>Project Document rules:<ul> <li>They are called \"Project document updates\" if output in section narrative</li> <li>They are called \"Project document\" if input in the section narrative</li> </ul> </li> <li>Project Management plan rules</li> <li>Sequencing rules</li> <li>Rules for handling tools and techniques</li> </ul> </li> </ul> <p>Pre-project Initiation</p> Info <p>What is a project?</p> <ul> <li>A project is temporary and unique. This is the best definition of what a project is.<ul> <li>Imagine you need to build a website for a department for internal use</li> <li>They will be the key stakeholders and will have specific requirements, budget and timelines</li> <li>Once the website is built to spec it will be handed over to the department and they will confirm or suggest changes</li> <li>The project will close once handed over</li> <li>This is essentially the entire cycle of the project:<ul> <li>Initiation</li> <li>Planning and execution</li> <li>Monitoring and control</li> <li>Formal closure</li> </ul> </li> <li>Post closure if the website needs more features this will be a fresh new project</li> </ul> </li> <li>Another important way to describe a project is progressive elaboration</li> <li>All projects create business value (tangible or non-tangible)<ul> <li>Tangible:<ul> <li>Stockholder value</li> <li>Market share</li> <li>Tools</li> <li>Other monetary assets</li> </ul> </li> <li>Non Tangible:<ul> <li>Brand recognition</li> <li>Trademarks</li> <li>Reputation</li> <li>Goodwill</li> </ul> </li> </ul> </li> <li>Most common reason a company would undertake a project are due to legal, social or regulatory requirements</li> <li>Specific factors that lead to project creation:<ul> <li>New technology</li> <li>Competitive forces</li> <li>Material issues</li> <li>Political changes</li> <li>Market demand</li> <li>Economic changes</li> <li>Customer requests</li> <li>Stakeholder demands</li> <li>Legal requirement</li> <li>Business process improvements</li> <li>Strategic business opportunities or business need</li> <li>Social need</li> <li>Environmental considerations</li> </ul> </li> </ul> <p>What is a program?     - A program is a set of related projects that are run in a co-ordinated way     - Orgs run programs because they have a set group of best practices or processes that can be utilized on all of them</p> <p>What is a portfolio?     - A portfolio is a set of unrelated programs and projects     - Project and Program management is interested in completing the projects the right way     - Portfolio management is about determining the right projects or programs to initiate based on business needs</p> Tip <ul> <li>Process group: Initiation</li> <li>Exam: Domain 3 - Business environment</li> <li>Key phrases:<ul> <li>Temporary</li> <li>Unique</li> <li>Project</li> <li>Program</li> <li>Portfolio</li> <li>Operations</li> </ul> </li> </ul> <p>What is project Management?</p> <ul> <li>In general, it is the project manager's job to utilize a set of best practices, tools and techniques, excellent communication, and coordination of all the moving parts to bring the result to successful completion.</li> <li>Most best practices are related to cost, schedule and scope</li> <li>Types of project management:<ul> <li>Predictive or waterfall project management<ul> <li>The end result is known or predicted</li> <li>The expected outcome is known</li> <li>There is a formal change control</li> <li>Requirements dont change midway</li> <li>Solutions are formulated, communicated and approved before implementation</li> <li>Plans are created well in advance</li> </ul> </li> <li>Adaptive or agile project management<ul> <li>The outcome is not known</li> <li>Change is welcome</li> <li>Teams are only 1 to 4 weeks into the cycle</li> <li>No front loaded planning. Decisions are taken at the last responsible moment</li> </ul> </li> </ul> </li> <li>Project Management lifecycles:<ul> <li>There are a few types of project lifecycles that can be applied depending on the project:<ul> <li>Predictive<ul> <li>Scope, budget, schedules are decided early on</li> </ul> </li> <li>Iterative<ul> <li>Scope is decided early but schedule and budgets are modified later</li> <li>Develop the product in repeated cycles</li> </ul> </li> <li>Incremental<ul> <li>Deliverables are added in iterations within a timeframe</li> </ul> </li> <li>Adaptive<ul> <li>Detailed scope is defined and approved before the iteration</li> <li>Highly change driven</li> </ul> </li> <li>Hybrid<ul> <li>Combination of predictive or adaptive </li> </ul> </li> </ul> </li> </ul> </li> <li>Project phases:<ul> <li>Each project will have a few phases</li> <li>Phases are a logical collection of activities that will produce a deliverable</li> <li>Phases may have these or more attributes:<ul> <li>Name: Phase 1, 2 etc</li> <li>Number: Total phases</li> <li>Duration: Set or expected duration</li> <li>Resource requirements: Required resources people, equipment</li> <li>Entrance criteria</li> <li>Exit criteria</li> </ul> </li> <li>Each phase will have gates which are basically ways to determine if one phase ends and other is supposed to start</li> <li>Its a way to decide how far we are in the lifecycle and what phase we are in</li> <li>It allows us to consider the preventive and corrective actions needed to be taken to reach the next phase</li> </ul> </li> <li>Project Management data and information:<ul> <li>Work performance data<ul> <li>Raw data collected from members e.g. \"How much time did you spend on it?\"</li> <li>The project manager gets this information and plugs them in the PMIS</li> </ul> </li> <li>Work performance information<ul> <li>The data is used to compare with original plan which generates information like scope, schedule, costs</li> <li>If the numbers are outside the planned limits then adaptive actions can be taken</li> <li>Information can also be used to forecast performance</li> </ul> </li> <li>Work performace reports<ul> <li>The information collected is communicated to stakeholders</li> </ul> </li> </ul> </li> <li>Project Selection techniques:<ul> <li>Why companies undertake projects?:<ul> <li>Market demand</li> <li>Customer request</li> <li>Business need or opportunity</li> <li>Technical advances</li> <li>Legal compliance</li> <li>Environment considerations</li> <li>Social need</li> </ul> </li> <li>Businesses need to validate the reasons to undertake the project</li> <li>The project must fit into the organizations strategic plan</li> <li>Business case is the main driver for the pre-project initiation.</li> <li> <ul> <li>Business analysts are involved during pre-project initiation</li> <li>They analyse the data and consider multiple financial models to determine ROI</li> <li>However, this is predictive and mostly has holes in it</li> <li>We might have a better chance to come close to the numbers when the project is planned and understood</li> <li>Hence a project manager might be involved in the selection process</li> <li>Analysis:<ul> <li>Used to narrow the gap between crunched and actual numbers</li> <li>Types:<ul> <li>Decision models<ul> <li>Cost-benefit analysis</li> <li>Scoring models</li> <li>Payback period</li> </ul> </li> <li>Economic models<ul> <li>Discounted Cash flow<ul> <li>Determining the future financial return worth</li> <li>Process:<ul> <li>Find the payback period</li> <li>Apply the discount rates and inflation</li> <li>Assumed amount of money the organization will recieve overall</li> </ul> </li> </ul> </li> <li>Net present value<ul> <li>Net gain or loss in each timeframe</li> <li>NPV analysis is the process of taking expenditures, net gains, and net losses for each potential year in an attempt to determine whether the project will return enough net revenue to keep up with the cost of capital over time.</li> <li>Tip: Always choose the highest NPV</li> </ul> </li> <li>Internal Rate of Return<ul> <li>IRR allows to calculate return without any external factors like inflation or cost of capital</li> <li>It considers time value of money</li> <li>**Tip: Always consider projects with highest IRR</li> </ul> </li> <li>Tip: Typically for exam the NPV is the key indicator of a profitable project</li> </ul> </li> <li>Constrained optimization<ul> <li>These are mathematical models used to determine project viability or profitable</li> </ul> </li> <li>Expert judgement<ul> <li>Utilize opnions from experts and stakeholders to determine projects</li> <li>Downside is groupthink which can be avoided by using the other models</li> </ul> </li> <li>Feasibility analysis<ul> <li>Using expert judgement to determine feasibility in addition to financial ROI</li> </ul> </li> </ul> </li> <li>Business case:         - The business case is the very first document that will be created         - It is comprehensive         - It includes the executive summary first but is usually written last         - The following header will also be included:             - Issues that the project is addressing             - The anticipated outcomes             - Any recommendations from the selection committee             - The business case analysis team             - The problem definition and statement that describes why the project is under consideration             - The organizational impact of the project             - Technology implementations             - Project overview and description             - Goals and objectives             - Expected project performance             - Any assumptions or constraints             - Major milestones             - Strategic alignment             - Cost-benefit analysis             - Alternatives analysis             - Approval signatures</li> </ul> </li> <li>Stakeholders:<ul> <li>key stakeholders:<ul> <li>Project management office (PMO)<ul> <li>Standardize the governance of projects accross the organization</li> <li>Oversee resources, mehodologies, tools and techniques</li> <li>Three types:<ul> <li>Supportive</li> <li>Controlling</li> <li>Directive</li> </ul> </li> <li>Main functions:<ul> <li>Supporting Project Managers</li> <li>Manage shared resources</li> <li>Determining project management methodologies</li> <li>Coaching and mentoring</li> <li>Monitoring compliance of policies, procedures by performing an audit</li> <li>Coordinating communications across projects.</li> <li>Make recommendations.</li> <li>Provide knowledge transfers.</li> <li>Terminate projects as needed.</li> </ul> </li> </ul> </li> <li>Change control board (CCB)<ul> <li>Analyzing change requests and determining if the change was required and the solution is sound</li> </ul> </li> <li>Sponsor<ul> <li>Pays and delegate the funding</li> <li>Champion and helps get it chosen</li> </ul> </li> <li>Functional manager</li> <li>Procurement administrator/vendors</li> <li>Customers/end users</li> </ul> </li> </ul> </li> <li> <ul> <li>There are many structures but the 2 main types are as follows:<ul> <li>Functional<ul> <li>The manager of each department is incharge not the project manager</li> <li>The project manager role might be temporary</li> <li>Someone in the department is assigned to expedite the process but not given actual designation</li> </ul> </li> <li>Matrix<ul> <li>Weak Matrix<ul> <li>Operates in a functional, hierarchical design</li> </ul> </li> <li>Balanced Matrix<ul> <li>The balanced matrix emphasizes an equal focus on operational, functional work, and project work.</li> </ul> </li> <li>Strong Matrix<ul> <li>The core project team helps the project manager plan and execute the work</li> <li>The project manager and project team are full time.</li> </ul> </li> </ul> </li> <li>Projectized<ul> <li>The project manager has full authority.</li> </ul> </li> </ul> </li> </ul> <p>Organizational Structures:</p> Functional Weak Matrix Balanced Matrix Strong Matrix Projectized No power Limited Power Some Power Incharge of project Complete control Functional manager in charge Functional manager in charge Functional manager in charge Incharge but may borrow resources Complete control of resources Expediter Co-ordinator Co-ordinator Project Manager Project Manager Part-time Part-time Could be full time Full time Full time </li> </ul> <p>Creating a business case:</p> <ul> <li>The Project Manager's role:<ul> <li>Managing the project team</li> <li>Solving problems</li> <li>Managing communication across multiple stakeholders</li> <li>The ability to collect the right requirements for scope</li> <li>The ability to create and manage a budget and a schedule</li> <li>The ability to identify, analyze, and remove threats and take advantage of opportunities</li> <li>Having an understanding of quality assurance and quality control as needed</li> <li>Effective planning skills across multiple knowledge areas</li> <li>Organizational skills</li> </ul> </li> <li>Skills:<ul> <li>Leadership</li> <li>Teambuilding</li> <li>Communication</li> <li>Active listening</li> <li>Consensus building</li> <li>Problem-solving</li> <li>Conflict resolution</li> <li>Negotiation skills</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Charters and stakeholders</p> Info <ul> <li>Politics, Power and Leadership<ul> <li>Project manager needs to:<ul> <li>Observe organizational landscapes in order to collect information about a project</li> <li>Determine project methodlogly, practices</li> <li>Assess project needs, complexity and magnitude</li> <li>Recommend a project execution strategy</li> <li>Recommend a project methodlogy</li> <li>Using iterative, incremental practices throughout the project (stakeholder engagement, risk, lessons learned)</li> </ul> </li> <li>Powers of a project manager:<ul> <li>Positional: formal authoritative power</li> <li>Informational: being in control of gathering information</li> <li>Referent: admiration due to credibility</li> <li>Situational: based on unique situations</li> <li>Personal: personal charisma</li> <li>Relational: networking, connections, alliances</li> <li>Expert: skills, experience etc.</li> <li>Reward-oriented: power to give praise, positive feedback</li> <li>Punitive: Negative consequence</li> <li>Ingratiating: act of flattery to win favor or co-operation</li> <li>Pressure-based: used in case compliance is not happening</li> <li>Guilt based: invoking obligation or sense of duty</li> <li>Persuasive: ability to move people to a desired course of action</li> <li>Avoiding: not participating or strategic avoidance</li> <li>Project managers are supposed to be proactive and intentional about power. Dont wait for it to be granted.</li> </ul> </li> <li>Leadership vs Management:<ul> <li>Leadership focus: relationships, inspiring trust, doing the right things and leading using relational power and innovation</li> <li>Management focus: systems and structure, short term goals, doing things right, bottom-line</li> </ul> </li> <li>Leadership styles:<ul> <li>Laissez-faire: (Allow to do):<ul> <li>Allow team members to set their own goals</li> <li>Let them know you are available as they want you to be</li> </ul> </li> <li>Transformational<ul> <li>Motivate</li> <li>Allow people to trust themselves</li> </ul> </li> <li>Transactional<ul> <li>Also called management by exception</li> <li>Focus on project's goals</li> <li>Determine rewards based on milestone met</li> </ul> </li> <li>Servant leadership<ul> <li>Agile considerations</li> <li>A leader with this philosophy puts other people first</li> <li>Focus on grow, learn, develop and practice autonomy</li> <li>Agile focuses on self-managed teams hence this is an important style</li> </ul> </li> <li>Charismatic<ul> <li>Natural talent, high energy, strong convictions</li> </ul> </li> <li>Interactional<ul> <li>Trifecta of Transactional, Transformational and charismatic</li> </ul> </li> </ul> </li> </ul> </li> <li>Project Integration:         - Project integration encompasses all the other knowledge ares         - It means bringing process, knowledge and people together         - Three levels of integration:             - Process level             - Cognitive level             - Context level</li> <li>Project charter: Goals and objectives:<ul> <li>Prior to creation of a project charter a business case is created</li> <li>Business case is an output of a business analysis:<ul> <li>Identification of scope</li> <li>Analysis of the situation - org goals, risks, success factors</li> <li>Decision criteria to assess course of action: required, desired, optional</li> <li>Business scenarios for alternative course of action: do nothing, do minimum work, do more than minimum work</li> <li>Success measures: Milestones, dependencies, roles &amp; responsibilities</li> <li>Evaluation: Statement of benefits and measurement criteria</li> </ul> </li> <li>Another document that is created is the Benefits management plan:<ul> <li>Its a way to bring all benefits under one roof</li> <li>Focuses on how and when the sponsor organization realize benefits</li> <li>Typically headers include:<ul> <li>Target benefits including NPV</li> <li>Strategic alignment</li> <li>Timeframe</li> <li>Owner who will be incharge of communicating these benefits</li> <li>Metrics</li> <li>Assumptions</li> <li>Risks to the realization of these benefits</li> </ul> </li> </ul> </li> <li>High Level requirements:<ul> <li>At the start of the project not much is known</li> <li>Hence it is expected to agree on high level resources and then elaborate progressively</li> <li>Progressive elaboration applies to cost, schedule and scope as well as risk, quality and resources</li> <li>High-level requirements often include the following:<ul> <li>The predicted result for the scope of work: A data center in Scottsdale, Arizona with some basic design specs.</li> <li>The results of the business case analysis: 1.5 million.</li> <li>High-level schedule expectations: 1 year.</li> <li>Mandatory milestones: Key date considerations to meet the high-level schedule.</li> <li>Constraints: Scope, time, cost, quality, risk, and resources.</li> <li>Pre-approved sellers or vendors: We're not even sure they are available or have the ability to work on this project yet.</li> <li>The selection committee: Those who make these decisions.</li> </ul> </li> </ul> </li> <li>Criteria of a Project Charter:<ul> <li>In case of a question regarding accepting a project without a charter the correct answer is \"Mention risks and reject the project\"</li> <li>A charter is created once everyone agrees on the items thusfar i.e. scope of work, business case etc.</li> <li>The charter includes known problems, opportunities, risks, time frames and overall budget</li> <li>Project charter is created after business case and benefits management plan is created</li> <li>Charter is written and signed by the sponsor and is a formal authorization to start a project</li> <li>It names the Project manager.</li> <li>Why it is important to have a project charter before kickoff:<ul> <li>Describes aggreed upon scope</li> <li>Assigns a Project Manager and allows authority change from sponsor to PM</li> <li>Identifies key stakeholders</li> <li>its a written aggreement to start the project</li> </ul> </li> </ul> </li> <li>Typical Headers in a Project Charter:<ul> <li>Executive Summary:</li> <li>Project Name:</li> <li>Authorities:</li> <li>Project Manager:</li> <li>Initiating authority:</li> <li>Business Need:</li> <li>Project Description:</li> <li>Product\\Service Characteristics:</li> <li>Project relationship to business need:</li> <li>Assumptions:</li> <li>Constraints:</li> <li>Risk events:</li> <li>Approval:</li> </ul> </li> <li>Agile Project Charter:<ul> <li>A project charter is needed in Agile as well but is more flexible</li> <li>It can change and be adapted based on changes </li> </ul> </li> <li>Typical Headers in an Agile project charter:<ul> <li>Who: Stakeholders</li> <li>What: high level description</li> <li>When: timelines </li> <li>Where: worksites</li> <li>Why: reason for project</li> <li>How: full agile or a tailored approach</li> <li>Buy-in: Signatures for official buy-in</li> </ul> </li> </ul> </li> <li>Charter ITTOs:<ul> <li>Inputs:<ul> <li>Business documents: Business case, benefits management plan</li> <li>Aggreements: SLAs, MOUs, contracts</li> <li>Enterprise environmental factors: stakeholder expectations, regulatory, culture</li> <li>Operational process assets: processes, monitoring and reporting methods, templates</li> </ul> </li> <li>Tools and Technologies:<ul> <li>Expert Judgement</li> <li>Data gathering:<ul> <li>Brainstorming</li> <li>Focus Groups</li> <li>Interviews</li> </ul> </li> <li>Iterpersonal &amp; team skills:<ul> <li>Conflict management</li> <li>Facilitation</li> <li>Meeting management</li> </ul> </li> <li>Meetings</li> </ul> </li> <li>Outputs:<ul> <li>Project Charter</li> <li>Assumption log</li> </ul> </li> </ul> </li> <li>Stakeholder Management<ul> <li>The main idea of this knowledge area is to identify and manage stakeholder satisfactions levels &amp; focus on effective communication</li> <li>The main tasks include:<ul> <li>Engaging stakeholders</li> <li>Analyzing stakeholders</li> <li>Categorizing stakeholders</li> <li>Engaging stakeholders by category</li> <li>Strategizing stakeholder communications</li> </ul> </li> <li>Key concepts:<ul> <li>Overview of concepts:<ul> <li>Identify, plan. manage and monitor stakeholder engagement</li> </ul> </li> <li>Details:<ul> <li>Engagement should begin as soon as the project is chartered</li> <li>Managed as a project objective</li> <li>Continuous communication</li> <li>Stakeholder list should be updated regularly as project moves through various phases</li> </ul> </li> <li>Emerging trends:<ul> <li>Identify all stakeholder instead of just a few ones</li> <li>Ensuring the team is involved</li> <li>Review stakeholder community regularly</li> <li>Consulting most affected stakeholders via \"co-creation\"</li> <li>Capture both positive and negative value of engagement</li> <li>Communication technology</li> </ul> </li> <li>Adaptive  Agile environments:<ul> <li>Aggressive transparent communication</li> <li>Client, user and developers work in a co-creation environment</li> </ul> </li> </ul> </li> </ul> </li> <li>Stakeholder analysis:<ul> <li>Outputs:<ul> <li>Identify the stakeholders and their interest in the outcome</li> <li>Rights: Legal or moral</li> <li>Ownership: knowledge or financial contribution or being a buffer</li> </ul> </li> <li>Determine the outputs and categorize them</li> <li>Data representation:<ul> <li>Power  Interest or Power  influence grid or Power  Impact grid<ul> <li>If a person has high power and high influence then manage them closely</li> <li>If a person has low power and low influence minimal communication should suffice</li> </ul> </li> <li>Stakeholder cube</li> <li>Salience model<ul> <li>Used in a complex network of stakeholders</li> </ul> </li> <li>Direction on influence<ul> <li>Also known as heirarchy of involvement</li> <li>Upward to senior management</li> <li>Downward to team</li> <li>Outward to suppliers</li> <li>Sideways to project managers</li> </ul> </li> <li>Prioritization</li> </ul> </li> </ul> </li> <li>Stakeholder register:<ul> <li>Ask the following questions to create a register:<ul> <li>How do you prefer to communicate? Email? Telephone? Meetings?</li> <li>How often would you like updates and status reports?</li> <li>Would you prefer to pick and choose which meetings to attend and if so, I won't put \"required\" on the invite?</li> <li>What are your needs and concerns about this project?</li> <li>What is the planned level of involvement you are expecting?</li> <li>What are your expectations of the deliverable, schedule, budget, and so on?</li> </ul> </li> </ul> </li> </ul> <p>Agile considerations</p> Info <p>Creating and leading a team</p> Info","tags":["Project-management"]},{"location":"Management/pmp-study/#section-2-project-management-processes","title":"Section 2 - Project Management Processes","text":"","tags":["Project-management"]},{"location":"Management/pmp-study/#section-3-revision","title":"Section 3 - Revision","text":"","tags":["Project-management"]},{"location":"Management/prince-notes/","title":"Prince 2 - Agile - Notes","text":"<p>Introduction</p> Info <ul> <li>Prince2 Agile describes ways to configure Prince2 to Agile projects</li> <li> <ul> <li> <p>Projects are temporary and differs from BAU work in significant ways:</p> Projects BAU Work Temporary Ongoing Team created Stable team Difficult Routine Uncertain Considerable certainity </li> </ul> <p>Prince2 and Prince2 Agile are meant for projects only.</p> <ul> <li>It is important to understand the distinction as some of the Agile ways need to be applied differently</li> <li>BAU work:<ul> <li>Repeatable, routine tasks performed by a technical team without the need of a Project Manager</li> <li>Usually enhancements are made to an existing product</li> <li>Short timelines and steady flow of work and a dedicated team</li> </ul> </li> <li>Project:<ul> <li>A temporary situation where a team is assembled to address a problem or opportunity</li> <li>Usually such situations cannot be handled as BAU</li> <li>Projects can be a collection of BAU items</li> <li>Highly uncertain</li> <li>Need to engage a lot of stakeholders</li> <li>It may be geographically spread out and might be part of a larger program</li> <li>Above all it needs to be managed by a Project Manager</li> </ul> </li> <li>BAU mode operates on fixed timeboxes.</li> <li>Time boxes are finite period when work is carried out to fulfil an objective</li> <li>Deadlines should not be moved. Timeboxes are used to prioritize the work inside the time duration</li> <li>Timeboxes can be of two types: low level(sprints) or high level(stages)</li> </ul> Note <ul> <li>Prince2 treats all agile frameworks as a collection of behaviors, concepts and techniques</li> <li>There are multiple agile frameworks some of which are IT only or are generic.</li> <li>Prince2 treats them as a collection and applies principles on top of it</li> </ul> Terms Examples Similar Terms Behaviors collaboration, self-organized, trust, blameless Principles, values, mindset Concepts prioritizing what you deliver, reduce waste, inspect and adapt Fundamentals Techniques Burndown chart, user stories, retrospectives Practices, tools </li> </ul> <p>Rationale of Prince2 + Agile</p> Info <ul> <li>Prince2 and Agile tend to complement each other well</li> <li>Prince2 excels in Project Direction and Project Management but weak in project delivery</li> <li>Agile excels in project delivery</li> <li>Thus combining both aspects provide a more holistic approach</li> <li>PRINCE2 Agile provides guidance for tailoring PRINCE2 to work in the most effective way in an agile context.</li> <li>It would be understandable to think that bringing more control and governance into the agile domain could prove counter-productive.</li> <li>However, PRINCE2 Agile represents a marriage that is based on the opposite view: that control and governance allow agile to be used in more situations such as those involving  multiple teams or complex environments.</li> </ul> <p>Prince2 flow using agile</p> Info <ul> <li>Lifecycle of a PRINCE2 project <pre><code>graph LR\nA([Pre-Project]) --&gt; B([Initiation])\nB --&gt; C([Subsequent Delivery stages])\nC --&gt; D([Final Delivery stage])\nD --&gt; E([Post-project])</code></pre></li> <li>Each of these stages can be represented as Agile processes by considering the following:<ul> <li>Plan, monitor &amp; control</li> <li>Behaviors</li> <li>Processes</li> <li>Products</li> </ul> </li> </ul> <p>Structure of Prince2</p> Info <ul> <li>Definition of a project:<ul> <li>A temporary organization created to deliver one or more products according to an agreed business case</li> <li>Prince2 should be only be used for projects where the temporary organization needs to be big and complex enough</li> <li>Otherwise setting up process controls can be an overhead</li> </ul> </li> <li>Project management cycle:     <pre><code>    graph LR\n    A[Plan] --&gt; B[Delegate];\n    B --&gt; C[Monitor];\n    C --&gt; D[Control];\n    D --&gt; A;</code></pre></li> <li>Definition of a program:<ul> <li>A temporary flexible organization created to oversee a set of related projects </li> <li>Setup to deliver outcomes and benefits in line with strategic objectives</li> </ul> </li> <li>Prince2 Structure:     <pre><code>    graph TD\n    A([Principles]);\n    B([Themes]);\n    C([Processes]);\n    D([Project Environment]);</code></pre></li> <li>Principles:<ul> <li>Set of guiding principles and best practices that need to be followed to ensure the project is Prince2</li> <li>There are 7 principles:<ul> <li>continued business justfication</li> <li>learn from experience</li> <li>defined roles and responsibilities</li> <li>manage by stages</li> <li>manage by exception</li> <li>focus on product</li> <li>tailor to suit projects</li> </ul> </li> </ul> </li> <li> <ul> <li>Aspects that must be continually addressed parallely</li> <li>7 themes:</li> </ul> <p>Themes:</p> Theme Answers Purpose Business case What? Organization Who? Quality What? Plans When? How? How much? Risk What if? Change What is the impact? Progress Where? (are we now and going in future) </li> <li> <p>Processes:</p> <ul> <li>Project management flow from pre-project to post-project stages</li> <li>Each stage has its own activities, products and responsibilities</li> </ul> </li> <li>Project Environment:<ul> <li>Consistent approach of managing projects embedded into orgs way of working</li> </ul> </li> </ul>"},{"location":"Management/scrum-fw/","title":"The SCRUM framework","text":""},{"location":"Management/scrum-fw/#agility-basics","title":"Agility Basics","text":"<ul> <li>The concept of Agile can be better understood using a stacey diagram.</li> <li>A stacey diagram shows the relationship between degree of certainity and degree of aggreement in a project</li> <li>The area in the graph is called a complex zone</li> <li>The only way to win in a complex zone is to achieve simplicity and move away from chaos</li> <li>Ways to do it?<ul> <li>Inspect and adapt</li> <li>Do it frequently</li> <li>Take effective and efficient decisions</li> </ul> </li> <li>In order to win you need to have 3 things:<ul> <li>A framework - Set of basic principles - Example Scrum</li> <li>Strategies - Inifinite - Sprint duration 3hrs</li> <li>Tools and Techniques - Infinite - Jira</li> </ul> </li> <li>Agile provides a set of values guiding principles for a winning team.</li> <li>Typically these are the values that a winning team will value.</li> </ul> <p>Agile manifesto  Agile principles </p>"},{"location":"Management/scrum-fw/#scrum-fundamentals","title":"Scrum Fundamentals","text":""},{"location":"Management/scrum-fw/#scrum-terms","title":"Scrum Terms","text":"Scrum Teams Scrum Events Scrum Artifacts Scrum Values Product Owner Sprint Product Backlog Focus Scrum Master Sprint Planning Sprint Backlog Respect Developers Daily Scrum Iteration Openness Sprint Review Courage Sprint Retrospective Commitment"},{"location":"Management/scrum-fw/#a-typical-scrum-setup","title":"A typical Scrum setup","text":"<ul> <li>The Product backlog refinement happens sometime during the current sprint </li> <li>The goal is to discuss the backlog items in terms of acceptance criteria, value etc.</li> <li>The backlog is then ready for the Sprint planning</li> <li>Product increment does not mean release</li> <li>Release strategy is completely dependent on the PO</li> <li>Scrum is an empirical process of Plan, do, check, and act activities</li> </ul>"},{"location":"Management/scrum-fw/#tools-techniques-and-strategies","title":"Tools, Techniques and Strategies","text":""},{"location":"Management/scrum-fw/#working-agreements","title":"Working Agreements","text":"<ul> <li>Mutually agreed activities and behaviours that make us achieve our goals effectively, efficiently as a team.</li> <li>Benefits:<ul> <li>sets clear expectations from each other</li> <li>accountability for each others behaviours</li> <li>successfully helps working toward a common goal</li> <li>handle conflicts</li> <li>creates effective and efficient teams</li> </ul> </li> <li>Who:<ul> <li>The scrum team can brainstorm on ideas to aggree on activities and behaviors</li> <li>Stakeholders and customers can be consulted for suggestions</li> </ul> </li> <li>When:<ul> <li>To be created immediately when the team is identified</li> <li>Can be updated during sprint retrospectives</li> </ul> </li> <li>Contents:<ul> <li>Values (FROCC)</li> <li>Ways to deal with conflict</li> <li>Activities and behaviors for achiving common goals like continuous improvements</li> <li>Individual commitments </li> <li>Purpose of the team</li> </ul> </li> </ul>"},{"location":"Management/scrum-fw/#common-terminologies","title":"Common Terminologies","text":"<ul> <li>Definition of Ready:<ul> <li>This is a list of items that tells if a PBI is ready to be considered for a sprint</li> <li>This is not a checklist where each item has to be done</li> <li>Examples:<ul> <li>All acceptance criterion are discussed and agreed</li> <li>All dependencies and risks are resolved or agreed resolution plan is in place</li> <li>All questions are answered</li> <li>Size of the PBI is small</li> <li>Assumptions clarified</li> <li>Prioritized and ordered</li> </ul> </li> </ul> </li> <li>Definition of Done:<ul> <li>List of all items that are needed to qualify an item as 100% done</li> <li>It includes all process, quality related items</li> <li>Evolves over time</li> <li>It is an ideal state that defines the work is done</li> <li>Examples:<ul> <li>Coding is complete</li> <li>Quality and functional checks complete</li> <li>Tests added and run</li> <li>Reviews done</li> <li>Documentation done</li> <li>Organization defined quality processes done</li> </ul> </li> </ul> </li> <li>Acceptance Criteria:<ul> <li>Mutually Aggreed Solution Statement</li> <li>It defines the functionality that needs implementation</li> <li>Examples:<ul> <li>Have a search functionality</li> <li>Show top 16 items to user</li> </ul> </li> </ul> </li> <li> <p>Release Strategy:</p> <ul> <li>Defined by the Product owner</li> <li>It describes how, what and when are we release product iterations to customers</li> </ul> <p>Types:</p> Time based - Fixed Time based - Cadence based Scope based The release date is fixed The release cadence is fixed The number of features is fixed 16.12 Daily, Weekly, Monthly etc. 75 features </li> </ul>"},{"location":"Management/scrum-fw/#estimations","title":"Estimations","text":"<ul> <li>There are 2 types of estimations:<ul> <li>Size estimations<ul> <li>These use reference points to estimate complexity of a story</li> <li>T-Shirt size : S, M, L, XL...</li> <li>Story points : 1, 2, 3, 5, 8 ...</li> <li>These are non linear estimates</li> </ul> </li> <li>Effort estimations<ul> <li>Estimates based on time needed to implement</li> <li>These are linear estimates</li> </ul> </li> </ul> </li> <li>Estimation criteria : Scope, Time, Cost, Quality</li> <li>Traditional methods of Fixed cost, scope, time and quality work only when we are in the \"simple zone\"</li> <li>Estimations are not commitments</li> <li>Modern methods suggest to expect uncertainity and assume atleast 1 is variable</li> <li>We need to estimate only the variables and assess the estimate against the risk</li> <li>After considering the variables if the risk is in the acceptable zone then we are good to go</li> <li>When to estimate:<ul> <li>The estimation happens in a Product backlog refinement event</li> <li>The idea is to break down large product backlog items into \"SMALL\" chunks</li> <li>Small enough to be I/A within the Sprint</li> <li>If the PBI is already \"SMALL\" then there is no reason to estimate</li> </ul> </li> <li>Common tools and techniques:<ul> <li>Planning poker:<ul> <li>Technique of estimation based on Size</li> <li>Team has got PBI to work on which they will estimate using a unit e.g. Story points</li> <li>The prerequisite is there should be a reference story or stories that deem a story to have certain story points</li> <li>And all members should ideally come up with the same story points</li> </ul> </li> </ul> </li> <li>During the Sprint planning only \"SMALL\" stories must be allowed</li> <li>Sprint planning has a layer of estimations  forecasts as well.</li> <li>It includes either one of the two:<ul> <li>Capacity based planning<ul> <li>Number of hours available vs number of hours required to finish the Sprint goal</li> </ul> </li> <li>Historical data based planning<ul> <li>Based on historical data how many PBIs have we completed</li> <li>That gives a range of the possible PBIs we can complete</li> <li>Again this is an estimation not committment</li> </ul> </li> </ul> </li> </ul>"},{"location":"Management/scrum-fw/#velocity","title":"Velocity","text":"<ul> <li>Ideally if all stories are small they can be given 1 story point</li> <li>Assuming this we will consider the velocity concept i.e. 1 story = 1 story point</li> <li>We need to calculate velocity by adding all historical story points and finding an average per sprint</li> <li>Use this velocity to forecast based on release strategy</li> <li>The concept is illustrated below: </li> </ul>"},{"location":"Management/scrum-fw/#kano-model-for-prioritizations","title":"Kano Model for prioritizations","text":"<ul> <li>The Kano Model is a tool for measuring customer satisfaction. </li> <li>It prioritizes features based on how customers react to their presence or absence.</li> <li>The Kano model suggests prioritization based on the effect it will have on the end user</li> <li>A graph is plotted between customer satisfaction and future state of the feature</li> <li>A line is plotted based on the customer satisfaction levels and the following pattern emerges:<ul> <li>Basic expectations : Bare minimum expectations</li> <li>Linear expectations : Keep giving more</li> <li>Neutral expectations : Does not matter if it exists or not</li> <li>Exciting zone : Whoa we were not expecting it</li> <li>Reverse zone : Feature existence is a problem</li> </ul> </li> <li>The kano graph is illustrated below: </li> </ul>"},{"location":"Management/scrum-fw/#burndown-charts-burn-up-charts","title":"Burndown Charts / Burn up charts","text":"<ul> <li>Burndown Charts<ul> <li>A burndown chart represents the remaining work on the Y axis and time elapsed on the X axis</li> <li>The connection of total remaining work point at the start of the time period and the end point of the time period serves as a line of average burndown</li> <li>A burndown chart is an Inspect and Adapt tool during the time period. After the time period its not so useful.</li> <li>The time period can be a Sprint or Release</li> </ul> </li> <li>Burnup charts<ul> <li>Represents the work done and usually approaches a target</li> <li>It has the same elements as burndown charts but tracks a target</li> </ul> </li> <li>When to use:<ul> <li>Use Burnup chart when tracking a target e.g. in a Fixed scope release strategy.</li> <li>Use Burndown chart to Inspect and adapt remaining work at any point in a time box.</li> </ul> </li> </ul>"},{"location":"Management/scrum/","title":"Scrum","text":""},{"location":"Management/scrum/#scrum-basics","title":"Scrum Basics","text":"<p>Explore \u2197</p>"},{"location":"Technologies/","title":"Technologies","text":""},{"location":"Technologies/#aws-essentials","title":"AWS Essentials","text":"<p>Explore \u2197</p>"},{"location":"Technologies/#prompt-engineering","title":"Prompt Engineering","text":"<p>Explore \u2197</p>"},{"location":"Technologies/#power-bi","title":"Power BI","text":"<p>Explore \u2197</p>"},{"location":"Technologies/#azure-essentials","title":"Azure Essentials","text":"<p>Explore \u2197</p>"},{"location":"Technologies/#snowflake-db","title":"Snowflake DB","text":"<p>Explore \u2197</p>"},{"location":"Technologies/aws-practitioner/","title":"AWS Basics","text":""},{"location":"Technologies/aws-practitioner/#introduction","title":"Introduction","text":"<ul> <li>Notes from the AWS practitioner essentials learning course ware from amazon.</li> <li>AWS is built on the client server model</li> <li>The client for example a web browser sends a request to the server for example a AWS EC2 cluster</li> <li>The server returns the information requested</li> <li>AWS works in a pay as you go model where you can add or remove resources as needed</li> </ul> <p><pre><code>graph LR\nA[Client] --&gt; B[Server];\nB --&gt; A;</code></pre> - Formal definition: Cloud computing is a on-demand delivery of IT resources over the internet with pay as you go pricing - There are 3 main types of deployment models:</p> CloudOnPremiseHybrid <ul> <li>Existing applications are moved to cloud</li> <li>New applications are built in cloud</li> <li>Run all parts of the application in the cloud</li> </ul> <ul> <li>Use virtualization and other technologies with own data centers</li> <li>Also called private cloud</li> </ul> <ul> <li>Connect parts of current applications to cloud</li> <li>Needed sometimes for better maintenance of the applications on OnPremise</li> <li>Regulatory compliance could also be a factor</li> </ul> <p>Eg. batch processing automatin is in cloud but all other aspects are OnPremise</p> <ul> <li>Benefits of cloud:<ul> <li>Variable costs over upfront costs</li> <li>No need to maintain own data centers</li> <li>Scale in and scale out based on demand. no guessing</li> <li>Build massive economies of scale. Aggregated use by multiple customers results in low pay as you go price</li> <li>speed and agility</li> <li>Global deployments in minutes</li> </ul> </li> </ul>"},{"location":"Technologies/aws-practitioner/#compute","title":"Compute","text":"<ul> <li> <p>EC2 - Elastic Compute Cloud:</p> <ul> <li>These are virtual servers which can be procured on demand instantly</li> <li>Amazon takes care of procuring, securing and enabling them on the internet</li> <li>Makes use of virtualization technology that helps spin up virtual servers</li> <li>A \"Hypervisor\" helps share resources of the host machine between virtual servers and isolating them</li> <li>This is also called multi-tenancy and is managed by AWS</li> <li>We can choose the following aspects when requesting EC2 instances:<ul> <li>OS</li> <li>Number of instances</li> <li>Applications that run on the server</li> <li>Networking preferences - public, private etc.</li> </ul> </li> <li>EC2 instances can be vertically scaled i.e. the capacity can be added if application is maxing out the server resources</li> </ul> </li> <li> <p>EC2 Instance types:</p> <ul> <li>Optimization is based on 3 main criterion:<ul> <li>Compute</li> <li>Memory</li> <li>Storage</li> </ul> </li> </ul> </li> </ul> Type Optimized for Examples Key notes General Purpose Balance between all aspects variety of workloads like app servers, game servers, S/M DBs useful when compute, memory and storage need is equivalent Compute Optimized high performance compute high performace web apps, dedicated game servers, batch processing Used in cases similar to general purpose but need  HPC Accelerated Compute Compute + Accelerator graphics processing, data pattern matching cases where CPU based compute does not meet requirements Memory Optimized Memory high performace databases Used when CPU processing needs large amount of data to be preloaded Storage Optmized Storage ditributed file systems, warehousing IOPS = I/O per second. Used when we need high IOPS <ul> <li>Pricing:</li> </ul> Plan Notes On-Demand Useful for unpredictable workloads that cannot be interrupted Reserved Instances Two types standard and convertible. Can be bought in 1y or 3y terms. need to specify type, size, tenancy, os, availability zone Savings Plan hourly spend commitment to an instance type and region for 1y or 3y term. Saves 72% as compared to on-Demand Spot Instances Bid on available resources at AWS for upto 90% off but can be taken away with a 2 min notice Dedicated Instances Physical servers that are fully dedicated for ones use. Most expensive <ul> <li>Scaling:<ul> <li>Scaling means starting out with the resources you need and designing your architecture to scale out or in based on demand</li> <li>Automatic scaling process is provided by Amazon EC2 auto-scaling service</li> <li>The service has 3 modes:<ul> <li>Dynamic scaling: Responds to changing demand</li> <li>Predictive scaling: Schedules resources based on predicted loads</li> <li>Hybrid: Both dynamic and predictive scaling can be used together for faster scaling</li> </ul> </li> <li>The scaling happens in a programatic way</li> <li>For auto scaling you need to specify:<ul> <li>Minimum requirements: Bare minimum required to run the application</li> <li>Desired requirements: Defaults to minimum if not specified</li> <li>Maximum requirements: Maximum in case of load</li> <li>In either case the charges are per use</li> </ul> </li> </ul> </li> <li>Load balancing:<ul> <li>If scaling solves the problem of overloading a single server load balancing solves the problem of distributing the load</li> <li>Elastic load balancing distributes the load based on least amount of outstanding requests</li> <li>The front end only refers to the Load balancer URL and hence decoupled from application servers and are unconcerned about scaling of compute resources</li> <li>ELB is auto scaled as well with no change to hourly rate</li> <li>ELB along with Auto scaling provide high availability and performance</li> </ul> </li> </ul> <p></p> <ul> <li>Messaging Queuing:<ul> <li>There are two main types of architectures:<ul> <li>Monolithic<ul> <li>Tightly coupled</li> <li>When one part fails other parts are impacted</li> </ul> </li> <li>Microservice based<ul> <li>Multiple independent components that talk to each other</li> <li>Loosely coupled</li> <li>Less prone to failures. Fault tolerant</li> </ul> </li> </ul> </li> <li>The services that enable communication that in turn enables fault tolerant architectures are:<ul> <li>SNS - Simple notification service<ul> <li>Pub/Sub model</li> <li>One shot publishing a message, notification, http request to multiple subscribing applications</li> </ul> </li> <li>SQS - Simple Queuing service<ul> <li>A queue where messages are stored until the recieving application processes it</li> </ul> </li> </ul> </li> <li>Both of these applications do not require the recieving application to be available</li> </ul> </li> <li> <p>Other compute products:</p> <ul> <li>AWS Lambda</li> <li>AWS Elastic Container Service</li> <li>AWS Elastic Kubernetes service</li> <li>AWS Fargate</li> </ul> <p>In order to decide which one to consider use the below decision matrix: </p> </li> </ul>"},{"location":"Technologies/aws-practitioner/#global-infra-and-reliability","title":"Global Infra and Reliability","text":"<ul> <li>AWS ditributes its data centers accross the world and calls it \"Regions\"</li> <li>Each region has multiple data centers and they are all connected with fiber optic connectivity</li> <li>This is done to provide highly available and fault tolerant infrastructure</li> </ul>"},{"location":"Technologies/aws-practitioner/#regions","title":"Regions","text":"<ul> <li>There are 4 main criterion to consider while choosing a region:<ul> <li>Compliance</li> <li>Proximity</li> <li>Feature availability</li> <li>Pricing</li> </ul> </li> </ul>"},{"location":"Technologies/aws-practitioner/#availability-zones","title":"Availability Zones","text":"<ul> <li>Availability zones are data centers or collection of them physically isolated withing a region</li> <li>As a best practice we should have resources deployed in 2 AZs at any time to be fault tolerant</li> <li>Any AWS service that is tagged \"Regionally scoped service\" is automatically regionally highly available e.g. Load balancer</li> </ul>"},{"location":"Technologies/aws-practitioner/#edge-locations","title":"Edge locations","text":"<ul> <li>These are locations with cached copy of the application and its data to serve nearby customers</li> <li>This is basically a CDN and is called Amazon CloudFront</li> <li>The networking is done using Amazon DNS service Route53</li> <li>In case the customers need a local copy that is installed in their buildings then use AWS Outposts</li> </ul>"},{"location":"Technologies/aws-practitioner/#how-to-provision-aws-resources","title":"How to provision AWS resources","text":"<p>How to interact:</p> <ul> <li>\"Management Console\"<ul> <li>Browser based console to manually manage and monitor resources</li> </ul> </li> <li>\"AWS CLI\"<ul> <li>A command line utility to script AWS commands and automate the management</li> </ul> </li> <li>\"SDKs\"<ul> <li>Programming language specific development kits to automate the management</li> </ul> </li> </ul> <p>How to provision:     - Using the interaction tools     - Elastic beanstalk         - EC2 based tool         - Takes code and desired configuaration and inputs and builds the infrastructure         - Provides visibility as well     - AWS Cloud formation         - IaC tool         - Can be used with a host of services not just EC2         - Uses text based configuration files to build resources         - Less prone to error</p>"},{"location":"Technologies/aws-practitioner/#networking","title":"Networking","text":""},{"location":"Technologies/aws-practitioner/#virtual-private-cloud","title":"Virtual Private Cloud","text":"<ul> <li>VPC is essentially a private network inside AWS</li> <li>Inside the VPC services can be arranged based on the requirements in buckets called subnets.</li> <li>There are 2 possible subnets:<ul> <li>Public </li> <li>Private</li> </ul> </li> <li> <p>Considering 3 possible scenarios based on requests:</p> <ul> <li>Public requests to AWS cloud via open internet<ul> <li>Use internet gateway</li> <li>E.g. Users requesting access to our site</li> </ul> </li> <li>Private requests to AWS cloud via VPN over open internet<ul> <li>Use virtual private gateway</li> <li>Data center connections, internal users contacting the network via a VPN</li> </ul> </li> <li>Private requests to AWS cloud via a dedicated network<ul> <li>Use AWS direct connect</li> <li>This reduces network costs and increases bandwidth</li> <li>Data center connections routed via direct connect locations and connecting via virtual private gateway</li> </ul> </li> </ul> <p></p> </li> </ul>"},{"location":"Technologies/aws-practitioner/#network-acls-and-security-groups","title":"Network ACLs and Security groups","text":"<ul> <li>The access to VPC is granted by gateways.</li> <li>But this only secures the perimeter</li> <li>The subnets inside the VPC are accessed via the network access lists (Network ACLs) of the gateway</li> <li>These are stateless firewalls which allow or deny access to the subnets based on a list</li> <li>The default ACL allows all inbound and outbound traffic</li> <li>Custom ACLs can be used to decide security on this level</li> <li>Once the packets are inside the subnet the EC2 instances need a security layer on their level as well</li> <li>This is called a Security group</li> <li>These are stateful systems that allow or deny packets to the EC2 instances</li> <li>Security groups are implicit deny which means only the allowed traffic is let in</li> </ul>"},{"location":"Technologies/aws-practitioner/#global-networking","title":"GLobal networking","text":"<ul> <li>When a customer tried to connect to say a company website it uses a DNS resolver</li> <li>The DNS resolver translates the request to an IP address and allows the connection to the website</li> <li>Amazon has Route 53 to provide DNS resolution</li> <li>It helps connect infrastructure inside and outside AWS as well</li> <li>It has certain strategies to achieve DNS resolution:<ul> <li>Geolocation based</li> <li>Geoproximity based</li> <li>Latency based</li> <li>Weighted round robin</li> </ul> </li> <li>Another service that provides Global networking is Amazon CloudFront which is a CDN</li> </ul>"},{"location":"Technologies/aws-practitioner/#storage-and-databases","title":"Storage and Databases","text":""},{"location":"Technologies/aws-practitioner/#block-storage","title":"Block storage","text":"<ul> <li>EC2 instances have associated block storage but they are ephemeral and data is tied to EC2 lifecycle</li> <li>To solve this problem amazon provides EBS or Elastic block storage</li> <li>These can be provisioned as per size needs and attached to EC2 and the data persists</li> <li>EBS allows to take regular snapshots of data so that disaster recovery is possible</li> <li>These backups are incremental i.e. only the data that is changed is backed up</li> <li>Stores data in an Availability zone</li> </ul>"},{"location":"Technologies/aws-practitioner/#simple-storage-service","title":"Simple Storage Service","text":"<ul> <li>Amazon stores the files as objects</li> <li>An object is the file itself, metadata and a key</li> <li>These objects are stored inside buckets (think folders)</li> <li>Maximum size of the object is 5TB</li> <li>Durability: 99.99999999%</li> <li>The objects can be stored in multiple tiers namely:<ul> <li>S3 standard</li> <li>S3 Infrequent Access</li> <li>S3 one zone Infrequest access</li> <li>S3 Intelligent tiering</li> <li>S3 Glacier Instant retrieval</li> <li>S3 Glacier Flexible retrival</li> <li>S3 Deep archive</li> <li>S3 outposts</li> </ul> </li> <li>Amazon also provides lifecycle policies that can control the tiers based on usage and number of days for example</li> </ul>"},{"location":"Technologies/aws-practitioner/#ebs-vs-s3","title":"EBS vs S3","text":"<ul> <li>If you have a single file where you need to do micro edits choose EBS</li> <li>If you have large files that need to be dealt with as discrete objects then use S3</li> </ul>"},{"location":"Technologies/aws-practitioner/#elastic-file-system","title":"Elastic File System","text":"<ul> <li>True managed Linux file system</li> <li>The system supports multiple read and write connections</li> <li>Scales automatically</li> <li>Regionally scoped. EBS is AZ scoped</li> <li>On-premises can access data using Amazon Direct connect</li> </ul>"},{"location":"Technologies/aws-practitioner/#relational-databases","title":"Relational databases","text":"<ul> <li>AWS supports relational databases to be installed on EC2 servers</li> <li>Supported databases:<ul> <li>PostgreSQL</li> <li>Oracle</li> <li>MySQL</li> <li>MariaDB</li> </ul> </li> <li>This is typically called lift-and-shift</li> <li>That way all of the current database activities and variables are available from the cloud</li> <li>It automates tasks such as hardware provisioning, database setup, patching, and backups.</li> <li>However, if we need a more managed approach then we can use Amazon Relational database System or RDS</li> <li>Amazon aurora is fully managed professional database system in the cloud</li> <li>It supports PostgreSQL and MySQL and provides the service at 1/10<sup>th</sup> the cost of other offerings</li> <li>It replicates six copies of your data across three Availability Zones and continuously backs up your data to Amazon S3.</li> </ul>"},{"location":"Technologies/aws-practitioner/#amazon-dyanmodb","title":"Amazon DyanmoDB","text":"<ul> <li>Dynamo DB is a fully managed serverless (no need to provision, patch or manage servers) database</li> <li>It is a non-relational and no-sql database specifically a key-value database</li> <li>It stores data as items and attributes</li> <li>It also has millisecond response times</li> <li>It is purpose built i.e. it only suits a certain work loads</li> </ul>"},{"location":"Technologies/aws-practitioner/#amazon-redshift","title":"Amazon Redshift","text":"<ul> <li>Managed data warehousing solution</li> <li>Warehousing is used when we try to answer the question \"what happened\"</li> <li>Used for big data analytics and BI analytics workflows</li> </ul>"},{"location":"Technologies/aws-practitioner/#amazon-data-migration-service","title":"Amazon Data migration service","text":"<ul> <li>Used when:<ul> <li>migrating onPremise data to cloud</li> <li>production to dev and test database</li> <li>consolidating databases</li> <li>continuous replication</li> </ul> </li> <li>Can be used to move data from different databases as well</li> <li>For example OP MySQL can be migrated to Cloud PostgreSQL</li> <li>The source database does not stop during migration and can still be accessed</li> </ul>"},{"location":"Technologies/aws-practitioner/#additional-databases","title":"Additional databases","text":"Service Type Use Remarks DocumentDB Database User profiles etc MongoDB workloads Neptune Database Social network, fraud detection Graph database Quantum Ledger Database compliance Audits Immutable database entries Elasticache Accelerator comes in redis and memcached flavours improves fetch from databases DynamoDB accelerator Accelerator improves DynamoDB fetches"},{"location":"Technologies/aws-practitioner/#security","title":"Security","text":""},{"location":"Technologies/aws-practitioner/#shared-responsibility-model","title":"Shared responsibility model","text":"<ul> <li>Security of the cloud - Owned by AWS<ul> <li>Physical infrastructure</li> <li>Servers</li> <li>Hypervisors</li> </ul> </li> <li>Security in the cloud - Owned by the customers<ul> <li>OS</li> <li>Applications</li> <li>Data</li> </ul> </li> </ul>"},{"location":"Technologies/aws-practitioner/#identity-access-management","title":"Identity &amp; Access Management","text":"<ul> <li>AWS provides a variety of options to control the access to the platform and its services<ul> <li>Root User:<ul> <li>Has access to everything</li> <li>Best practice is to add MFA as soon as 1<sup>st</sup> login</li> <li>Create new users based on tasks and assign permissions and use the created users to do tasks</li> </ul> </li> <li>Users, User groups, policies:<ul> <li>Root can create users and add them to user groups</li> <li>The activities they can perform are governed by policies</li> <li>Policies define the activities users or user groups can perform, resources they can access </li> </ul> </li> <li>Roles:<ul> <li>Allows users to assume temporary responsibility and do tasks</li> </ul> </li> </ul> </li> </ul> <pre><code>graph TD\nA[root user] --&gt;|policies| B[user 1];\nA --&gt;|policies| C[user 2];\nA --&gt;|policies| D[user group 1];\nD --&gt;|inherited policies| E[user 4];\nD --&gt;|inherited policies| F[user 5];\nA --&gt;|temporary responsibility| G[user 5 role 1]</code></pre>"},{"location":"Technologies/aws-practitioner/#aws-organizations","title":"AWS Organizations","text":"<ul> <li>AWS organizations is a service that allows grouping and managing AWS accounts </li> <li>It allows to set SCPs (service control policies) on individual accounts and group of accounts (Organizational units)</li> </ul> <pre><code>graph TD\nA[AWS Organization] --&gt;|SCPs| B[Account 1];\nA --&gt;|SCPs| C[Account 2];\nA --&gt;|SCPs| D[Organization Unit];\nD --&gt;|inherited SCPs| E[Account 3];\nD --&gt;|inherited SCPs| F[Account 4];</code></pre>"},{"location":"Technologies/aws-practitioner/#compliance","title":"Compliance","text":"<ul> <li>AWS has a shared responsibility model</li> <li>It completes part of the compliance requirements on its own</li> <li>The services and data built on top of AWS the compliance needs to be done by the company</li> <li>They can use the compliance functionality themselves or use the exisitng features in AWS</li> <li>All compliance reports can be accessed via \"AWS Artifacts\"</li> <li>Compliance center is a one stop solution to get all information related to compliance requirements for various use cases</li> </ul>"},{"location":"Technologies/aws-practitioner/#ddos","title":"DDos","text":"<ul> <li>A well architected system is already capable of handling some types of DDos attacks</li> <li>For example, security groups and ALB can take care of UDP flood and SLow loris type attacks</li> <li>AWS Shield is a service that can be used to protect against sophesticated attacks</li> <li>It has 2 modes:<ul> <li>Standard</li> <li>Advanced</li> </ul> </li> <li>It has a WAF web application firewall that takes care of the bad actor signatures and has ML capabilities</li> </ul>"},{"location":"Technologies/aws-practitioner/#additional-services","title":"Additional services","text":"<ul> <li>KMS - Key management systems that provides encryption at rest and transit</li> <li>Inspector - Regular security scans</li> <li>Guard duty - Proactive threat detection with continuous monitoring</li> </ul>"},{"location":"Technologies/aws-practitioner/#monitoring-and-analytics","title":"Monitoring and Analytics","text":""},{"location":"Technologies/aws-practitioner/#cloudwatch","title":"CloudWatch","text":"<ul> <li>Amazon cloudwatch collects metrics from host of services and helps show them in one place</li> <li>It allows to set alarms based on data points to trigger actions</li> </ul>"},{"location":"Technologies/aws-practitioner/#cloudtrail","title":"Cloudtrail","text":"<ul> <li>Making changes to the system in AWS is API driven</li> <li>Cloudtrail helps to identify changes based on these API calls</li> <li>It records who, what, when, how aspects of the change for effective audit</li> <li>CloudWatch Insights is a service we can use to detect unusual activity and get alerts</li> </ul>"},{"location":"Technologies/aws-practitioner/#trusted-advisor","title":"Trusted Advisor","text":"<ul> <li>Web service that provides information based on best practices of using AWS</li> <li>It provides information on 5 pillars:<ul> <li>Cost Optimization</li> <li>Performance</li> <li>Security</li> <li>Fault tolerance</li> <li>Service limits</li> </ul> </li> <li>Useful to avoid high costs and optimize for best use</li> </ul>"},{"location":"Technologies/aws-practitioner/#pricing-and-support","title":"Pricing and Support","text":"<ul> <li>AWS offers free tier which are either forever free, 12 month free or limited trial free types</li> <li>Pricing model is either:<ul> <li>Pay per use</li> <li>Reduced pricing for commitment</li> <li>Redeuced pricing for volume based usage</li> </ul> </li> <li>AWS has a billing dashboard where you can view MTD view of the resource usage</li> <li>You can also check Billing info</li> <li>AWS offers consolidated billing for users of AWS Organizations</li> <li>They can get simplified billing, share premium savings features withing accounts and the service is free</li> <li>AWS provides a way to create budgets based on actual or forecasted usage and set alerts</li> <li>Its called AWS Budgets</li> <li>AWS cost explorer lets you analyze the past data for cost.</li> <li>You can create a dashboard and filter by tags to analyze the data</li> <li>AWS offers support in the following tiers:<ul> <li>Basic - Free - limited trusted advisor checks </li> <li>Developer - Basic + best practice guidance etc.</li> <li>Business - This and above includes AWS Trusted advisor</li> <li>Business On-Ramp - This and above includes TAM (Technical Account Manager)</li> <li>Business Enterprise</li> </ul> </li> <li>AWS Marketplace lets you search for tools and apps built on AWS for 1 click solutions</li> </ul>"},{"location":"Technologies/aws-practitioner/#migration-and-innovation","title":"Migration and Innovation","text":""},{"location":"Technologies/aws-practitioner/#aws-cloud-adoption-framework","title":"AWS Cloud Adoption Framework","text":"<ul> <li>AWS professional services team has created a framework that helps people migrate to the cloud</li> <li>It has 6 pillars and results into an action plan <pre><code>graph LR    \nA[Cloud Adoption Framework] --&gt; B[Business];\nA --&gt; C[People];\nA --&gt; D[Governance];\nA --&gt; E[Platform];\nA --&gt; F[Security];\nA --&gt; G[Operations];\nB --&gt; H[Business, Finance managers];\nC --&gt; I[HR, Staffing];\nD --&gt; J[CIO, program managers, enterprise architects]\nE --&gt; K[CTO, Solution architects]\nF --&gt; L[CISO, IT Security Managers]\nG --&gt; M[IT Ops managers]</code></pre></li> </ul>"},{"location":"Technologies/aws-practitioner/#6-rs-of-migration","title":"6 Rs of Migration","text":"<pre><code>graph LR\nA[6 Rs] --&gt; |Lift and shift| B[Rehosting]\nA --&gt; |Lift tinker and shift| C[Replatforming]\nA --&gt; |Remove apps not needed| D[Retire]\nA --&gt; |Keep for a limited time| E[Retain]\nA --&gt; |New infrastructure for new possibilities| F[Repurchase]\nA --&gt; |Changing code to achieve new things|G[Refactor]</code></pre>"},{"location":"Technologies/aws-practitioner/#snow-family","title":"Snow Family","text":"<ul> <li>Family of physical devices used to transport data from OP to on cloud</li> <li>The devices are:<ul> <li>Snow Cone<ul> <li>Upto 14TB of storage</li> </ul> </li> <li>Snow Ball<ul> <li>Storage optimized<ul> <li>80TB of data</li> <li>40cpus</li> </ul> </li> <li>Compute optimized<ul> <li>80TB</li> <li>104 vcpus</li> </ul> </li> </ul> </li> <li>Snow Mobile<ul> <li>Exa bite container truck</li> </ul> </li> </ul> </li> </ul>"},{"location":"Technologies/aws-practitioner/#innovation","title":"Innovation","text":"<ul> <li>Serverless<ul> <li>AWS Lambda</li> </ul> </li> <li>Machine Learning<ul> <li>AWS SageMAker</li> </ul> </li> <li>AI<ul> <li>Augmented AI</li> <li>Code Whisperer</li> </ul> </li> </ul>"},{"location":"Technologies/aws-practitioner/#cloud-journey","title":"Cloud Journey","text":""},{"location":"Technologies/aws-practitioner/#aws-well-architected-framework","title":"AWS Well Architected Framework","text":"<ul> <li>The framework has 6 main pillars</li> <li>It is a collection of best practices that helps design or evaluate current AWS systems</li> <li>The framework either needs to be evaluated by a Solution architect or needs to be evaluated using a self service tool</li> </ul> <pre><code>graph TD\nA[AWS WAF] --&gt; B[Operational Excellence]\nA --&gt; C[Security]\nA --&gt; D[Cost Optimization]\nA --&gt; E[Reliability]\nA --&gt; F[Performance Efficiency]\nA --&gt; G[Sustainability]</code></pre>"},{"location":"Technologies/azure-practitioner/","title":"Azure Practioner","text":""},{"location":"Technologies/azure-practitioner/#core-azure-concepts","title":"Core Azure Concepts","text":"<ul> <li>Azure Subscription<ul> <li>Its an account where all your services will be stored</li> </ul> </li> <li>Resource groups<ul> <li>Its a bucket in which you can store your services</li> <li>Logically grouping them and providing permissions as needed</li> <li>Two resource groups cannot share a resource</li> </ul> </li> <li>Tagging<ul> <li>Allows to tag services by a name</li> <li>The tags can be based on application or departments etc</li> <li>Very nifty when analyzing usage</li> </ul> </li> <li>Azure Resource Manager<ul> <li>API based resource manager</li> <li>The management can be done in a few ways viz.:<ul> <li>ARM templates</li> <li>Azure portal</li> <li>Azure CLI</li> <li>Azure via powershell (windows only)</li> </ul> </li> <li>Use ARM templates for IaC requirements</li> <li>Use Azure portal for visual management</li> <li>Use Azure CLI when ARM templates get complicated and need better declarative scripts</li> <li>Use Azure CLI for building cross platform scripts</li> </ul> </li> <li>Azure Regions and Availability Zones<ul> <li>Azure offers multiple regions accross the globe</li> <li>Most regions have something called as a Region pair which can be used to introduce redundancy to the system</li> <li>While choosing a region consider the following:<ul> <li>Region closest to your current data center if giving up OnPremise </li> <li>Region farther away from your current data center if keeping the OnPremise setup</li> <li>Region closer to your majority user base</li> <li>Region in the middle of your majority user base to provide same latency to all</li> </ul> </li> <li>Each region has 1 or more availability zones</li> <li>Each availability zone is a collection of 1 or more data centers</li> <li>These provide fault tolerance by making use of fault and update domains</li> <li>AZs are separated physically</li> <li>AZs along with Region pairs customers can have lots of redundancy introduced in the system</li> </ul> </li> </ul>"},{"location":"Technologies/azure-practitioner/#azure-app-services","title":"Azure App Services","text":"<ul> <li>Azure App service is basically an instance where you can host your application code</li> <li>The application may include Rest APIs, mobile backend etc</li> <li>It supports a limited number of languages like:<ul> <li>.Net</li> <li>Java</li> <li>Python</li> <li>C#</li> </ul> </li> </ul>"},{"location":"Technologies/azure-practitioner/#azure-storage","title":"Azure Storage","text":"<ul> <li>Storage<ul> <li>Storage is a service which allows to store unstructured data like images, videos, documents etc.</li> <li>It comes in 2 flavours:<ul> <li>Blob storage<ul> <li>Used to store large images or videos</li> <li>Or in cases where media needs to be available over https</li> <li>Or in case there is a need for backups</li> <li>Cheaper than file storage</li> </ul> </li> <li>File storage<ul> <li>Primarily used in cases where existing applications use SMB protocol and the whole application cannot be re architected</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Blob storage details <pre><code>graph LR\nA[Blob storage] --&gt; |Levels of access|B[Access Tiers]\nA --&gt; |Levels of performance|C[Performance Tiers]\nB --&gt; |Easy access, cost high|D[Hot]\nB --&gt; |Moderately fast access, cost medium|E[Cold]\nB --&gt; |Slow access, cost low|F[Archive]\nC --&gt; |regular work|G[Standard]\nC --&gt; |High performace work|H[Premium]</code></pre></p> </li> <li> <p>Cosmos DB</p> <ul> <li>Cosmos DB is Azure No-SQL offering and comes in many flavours <pre><code>graph LR\nA[CosmosDB] --&gt; B[Document]\nA --&gt; C[Key-Value pair]\nA --&gt; D[Wide Column]\nA --&gt; E[Graph]</code></pre></li> <li>It supports multiple access APIs like Cassandra API and Mongo API</li> <li>Its not a replacement for SQL server unless the application is largely refactored</li> </ul> </li> <li> <p>Azure SQL Databases</p> <ul> <li>Deployed or managed SQL servers in the cloud</li> </ul> <p><pre><code>graph LR\nA[Databases] --&gt; B[Hosted infrastructure]\nA --&gt; C[Managed SQL]</code></pre> - Deployment options: <pre><code>graph LR\n\nA[Single Databases]\nB[Hyperscale Databases]\nC[Serverless Databases]\nD[Elastic Pool]</code></pre></p> </li> </ul>"},{"location":"Technologies/azure-practitioner/#azure-functions","title":"Azure Functions","text":"<ul> <li>Azure Functions work in the same way as AWS Lambda</li> <li>These are serverless applications and we just need to provide code, trigger and configuration</li> <li>Azure functions need to be hosted in a Function app</li> <li>Function app needs the application to be coded in one language</li> <li>Azure functions can be created as a file or in some cases the files are autogenrated using annotations</li> </ul> <p>Triggers:</p> <pre><code>graph LR\nA[Function triggers] --&gt; B[Timer Trigger]\nA --&gt; C[Queue Trigger]\nA --&gt; D[CosmosDB Triggers]\nA --&gt; E[Event Grid Triggers]\nA --&gt; F[Event Hub Triggers]</code></pre> <ul> <li>Complex function workflows can be implemented in a cleaner way with Durable function</li> </ul>"},{"location":"Technologies/azure-practitioner/#batch","title":"Batch","text":"<ul> <li>Azure batch is used when we need to perform the same operation on a large number of objects</li> <li>For example a social network want to compress videos uploaded to their app</li> <li>Here we need to follow the following: <pre><code>graph LR\nA[Videos] --&gt; |Add| B[Azure Storage]\nC[Scripts] --&gt; |Add| B\nB --&gt; D[Run Batch]</code></pre></li> </ul>"},{"location":"Technologies/azure-practitioner/#kubernetes-and-containers-in-azure","title":"Kubernetes and Containers in Azure","text":"<ul> <li>Azure Kubernetes Service</li> <li>Azure Container Registry</li> </ul>"},{"location":"Technologies/azure-practitioner/#securing-applications","title":"Securing Applications","text":"<ul> <li>Authentication is the process of proving the identity based on some factors</li> <li>Authorization is the process of determining wheter an action is allowed to do by a user</li> <li>Authorization happens after authetication</li> <li>Azure provides ways to authentic identity using some identity providers which is the recommended way to deal with identity</li> <li>Authorization can be:<ul> <li>Role based</li> <li>Claims based - more flexible and recommended</li> </ul> </li> </ul> <p>Azure Key Vault: - Similar to AWS KMS - Stores keys and other sensitive information - Integrates with other aspects of the eco system as well</p>"},{"location":"Technologies/azure-practitioner/#monitoring-applications","title":"Monitoring Applications","text":"<ul> <li>Azure Monitor helps track telemetry data and gain insights on the application use</li> <li>It gets the data in 2 formats: metrics and logs</li> <li>The data can be analyzed reactively using the logs or metrics</li> <li>Azure monitor can also help set alerts based on usage</li> </ul> <p>Application Insights - Application Insights provides usage information of the deployed service - We only need to add the package to our code and it starts collecting and aggregating metrics for us   </p>"},{"location":"Technologies/azure-practitioner/#optimizing-your-applications","title":"Optimizing your applications","text":"<ul> <li> <p>Azure Redis</p> <ul> <li>Redis cache based service from Azure</li> <li>It supports multiple caching strategies like cache aside, expiration, user-session caching etc.</li> </ul> </li> <li> <p>Azure CDN</p> <ul> <li>Uses inteligent routing</li> <li>Image compression</li> <li>Ability to geo-filter requests</li> </ul> </li> </ul>"},{"location":"Technologies/data-science-gsdc/","title":"Data Science","text":"<p>Notes from the \"Certified Data Science Professional\" GSDC certification training </p> <p>Statistics</p> Info <ul> <li>Statistics - Science of numerical data collected, analysed, interpret and present data</li> <li>Types:<ul> <li>Descriptive Stats:<ul> <li>Used to describe data</li> <li>Types:<ul> <li>Measure of central tendency: <ul> <li>mean : Average</li> <li>median : Central value</li> <li>mode : Most often value</li> </ul> </li> <li>Measure of spread: range, standard deviation</li> </ul> </li> </ul> </li> <li>Prescriptive Stats:<ul> <li>Used to make decisions or predictions</li> <li>Uses probability to determine confidence</li> </ul> </li> </ul> </li> <li>Dispersion using quartiles:<ul> <li>Percentile means the data set is divided in 100 parts</li> <li>Quartile means the data set is divided in 4 parts</li> <li>Outliers:<ul> <li>An observation is unusual if:<ul> <li>it is less than 1.5 times IQR below first quartile</li> <li>it is more than 1.5 times IQR 3<sup>rd</sup> quartiles</li> </ul> </li> </ul> </li> </ul> </li> <li>Types of Data:     | Type | Examples |     | ---- | -------- |     | Structured | RDBMS, CSV |     | Semi-structured | XML, JSON |     | Unstructured | Images, video, IOT, health |</li> <li>Univariate analysis<ul> <li>Single variable</li> </ul> </li> <li>Bivariate<ul> <li>Dependent variable is a function of independent variable</li> </ul> </li> <li>Multivariate<ul> <li>More than one or 2 variables used in analysis</li> </ul> </li> <li>Code samples:     <pre><code>    import numpy as np\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import statistics as stats\n    import pandas as pd\n\n    # Create array\n    a = np.array([1,4,5,4,8])\n\n    # Finding mean using numpy module\n    a.mean()\n    ages = np.random.randint(18, 90, 800)\n    # Finding mode \\ median using stats module\n    stats.mode(ages)\n    stats.median(ages)\n\n    # creating a set of random data with normal value\n    incomes = np.random.normal(27000, 15000, 10000)\n    np.mean(incomes)\n\n    # plotting a histogram\n    plt.hist(incomes, 100, ec='white')\n\n    # Finding variance\n    variance = a1.var()\n\n    # finding standard deviation\n    std = a1.std()\n\n    # Box plot, percentile, quartiles, inter quartile range\n\n    a1 = np.array([85, 100, 95, 92, 99, 103, 97, 110, 99, 102, 120, 90, 94])\n    sns.boxplot(a1)\n\n    q1 = np.percentile(a1,25)\n    q3 = np.percentile(a1,75)\n\n    iqr = q3-q1\n\n    lower = q1 - (1.5*iqr)\n    upper = q3 + (1.5*iqr)\n\n    # saving figure in a location\n    plt.plot(a1)\n    plt.saveFig('location on disk')\n\n    # reading data from a file\n    fileData = pd.read_csv('location')\n    file.head() # gives data from a file\n\n    # plotting data from columns\n    plt.plot(fileData['columnName'], fileData['columnName'])\n\n    # counting frequency of a value in a column\n    fileData.&lt;ColumnName&gt;.value_counts()\n\n    # creating a heatmap using seaborn\n    # This specific case is used to check null value\n    sns.heatmap(fileData.isnull())\n\n    # finding co-relation between various fields\n    sns.heatmap(fileData.corr(),cmap=\"coolwarm\")\n\n    # Pie plots\n    cities = [\"Pune\", \"Mumbai\", \"Delhi\"]\n    engineers = [230,333,421]\n    plt.pie(engineers, labels=cities, autopct='%.2f%%')\n\n    # PLotting a value plot\n    sns.valueplot(x='ColumnName', hue='ColumnName', data = fileData)\n</code></pre></li> </ul>"},{"location":"Technologies/power-bi/","title":"Power BI","text":""},{"location":"Technologies/power-bi/#intro-to-bi","title":"Intro to BI","text":"<ul> <li>Business Intelligence or BI is using data to make better decisions</li> <li>Better decisions to take the business forward</li> <li>Examples:<ul> <li>ways to attract new or retain old customers</li> <li>competitor analysis</li> <li>what is driving profits?</li> <li>what expenses can be diminished?</li> </ul> </li> <li>Key concepts:<ul> <li>Domain<ul> <li>The context in which BI is applied</li> <li>Ex. std business functions or depts like sales, marketing etc.</li> <li>Helps narrow focus</li> </ul> </li> <li>Data<ul> <li>Next step after selecting a domain is selection of relevant data</li> <li>That means finding sources of relevant data:<ul> <li>Internal Data<ul> <li>Data generated within the boundaries of an org</li> <li>net revenues, units produced, sales etc.</li> </ul> </li> <li>External Data<ul> <li>BI is most effective when internal data is combined with external data</li> <li>data created outside the boundaries of an org</li> <li>Eg. global economic performance, census info, competitor prices etc.</li> </ul> </li> </ul> </li> <li>Also types of data:<ul> <li>Structured<ul> <li>Data that conforms to a specific structure with rows and columns</li> <li>Mostly relational database systems and database standards (ODBC, OLE DB) fall in this category</li> </ul> </li> <li>Unstructured<ul> <li>Data that cannot be organized into standard rows and columns</li> <li>Videos, audio, images, text</li> <li>word, pdfs, emails, social media posts</li> <li>these are most difficult to consume and analyze</li> <li>stored as BLOBS (binary large objects) or as a file in file systems (NTFS, HDFS)</li> <li>No-SQL databases also fall under this category</li> </ul> </li> <li>Semi-structured<ul> <li>Structured data but not conforming to row/column standard</li> <li>Delimited text files, XML, markup languages, JSON, EDI</li> <li>They have self-defining structure which makes them easy to consume and analyze but harder than true structured data</li> <li>data access protocols like OData / REST also fall in this category</li> </ul> </li> </ul> </li> <li>BI tools are optimized for handling structured and semi-structured data</li> <li>BI tools are designed to ingest semi-structured data sources and transform them to structured</li> </ul> </li> <li>Model<ul> <li>A data model refers to the way in which one or more data sources are organized</li> <li>The organization is done to support analysis and visualization</li> <li>Key steps:<ul> <li>Organizing<ul> <li>Organzing is done by establishing how the multiple data sources relate to one another</li> <li>This helps model become a cohesive whole</li> <li>For example the relation between a CRM data and ERP data can be done based on customer name</li> </ul> </li> <li>Transforming and cleansing<ul> <li>It is almost always necessary to clean the source data</li> <li>Duplicates, trailing spaces, spelling mistakes, missing data</li> <li>Transforming and cleansing tech are often called ETL and have special tools</li> </ul> </li> <li>Defining and cetegorizing<ul> <li>Defining data types on the source data eg. date, time, datetime etc.</li> <li>Defining catergories in each data type for example a text can be a url</li> <li>These provide ways to establish what analysis can be done on each type</li> </ul> </li> </ul> </li> </ul> </li> <li>Analysis<ul> <li>Grouping data, simple aggregations, sums, counts and averages</li> <li>sPecialized calculations to identify trends, correlations and forecasting</li> <li>Creating KPIs</li> <li>There are tons of tools to do the analysis. R, Python and SQL are top ones along with Excel.</li> </ul> </li> <li>Visualization<ul> <li>Visualizations are tables, matrices, pie charts, bar graphs, and other visual displays that help provide context and meaning to the analysis</li> <li>Business intelligence tools allow multiple individual tables and charts to be combined on a single page or report.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Technologies/power-bi/#power-bi-ecosystem","title":"Power BI ecosystem","text":"<ul> <li>Power BI is a collection of interralted tools and services that form a complete BI ecosystem.</li> <li>It includes tools for modelling, analysis and visualization</li> <li>The ecosystem can be broken down into following categories:<ul> <li>Core - Power BI specific<ul> <li>Power BI Desktop:<ul> <li>Free, Windows based app that installs on a local computer</li> <li>Its primary tool to ingest, cleanse, transform data, combine into models</li> <li>Analyze and visualize using calculations, visualizations, and reports</li> <li>Once reports are created in Power BI desktop they are published to Power BI service</li> </ul> </li> <li>Power BI service:<ul> <li>Cloud-based SaaS online platform</li> <li>light report editing, sharing, collaborating, and viewing reports</li> </ul> </li> </ul> </li> <li>Core - non Power BI specific<ul> <li>Data Query - Data connectivity and transformation</li> <li>DAX - programming language for Power BI</li> <li>On Premise data gateway - facilitate access from Power BI service to data sources</li> <li>SSAS - Ability to build models</li> <li>MS Appsource - Marketplace to find apps, add-ins and extensions to sotwares like Power BI</li> </ul> </li> <li>Non-core - Power BI specific<ul> <li>Report Server</li> <li>Embedded</li> <li>Mobile application</li> <li>Mixed reality</li> </ul> </li> <li>Natively integrated MS tech<ul> <li>Office 365, Excel, Microsoft Flow, Power Apps, Visio, Azure ML, Report builder</li> </ul> </li> <li>Extended ecosystem<ul> <li>Large ecosystem of 3<sup>rd</sup> party tools and add-ons</li> </ul> </li> </ul> </li> </ul>"},{"location":"Technologies/power-bi/#power-bi-desktop","title":"Power BI Desktop","text":""},{"location":"Technologies/power-bi/#connecting-and-shaping-data","title":"Connecting and Shaping Data","text":""},{"location":"Technologies/power-bi/#data-models-and-calculations","title":"Data Models and Calculations","text":""},{"location":"Technologies/power-bi/#unlocking-insights","title":"Unlocking Insights","text":""},{"location":"Technologies/power-bi/#final-report","title":"Final Report","text":""},{"location":"Technologies/power-bi/#publishing-and-sharing","title":"Publishing and Sharing","text":""},{"location":"Technologies/power-bi/#using-reports-in-the-service","title":"Using reports in the service","text":""},{"location":"Technologies/power-bi/#dashboard-apps-security","title":"Dashboard, Apps &amp; security","text":""},{"location":"Technologies/power-bi/#gateways-and-refreshing-datasets","title":"Gateways and refreshing Datasets","text":""},{"location":"Technologies/prompt-engg/","title":"Prompt Engineering","text":""},{"location":"Technologies/prompt-engg/#principles-of-prompting","title":"Principles of prompting","text":"Explore"},{"location":"Technologies/prompt-engg/#introduction","title":"Introduction","text":"<ul> <li> <p>What is a prompt?</p> <ul> <li>A prompt is an input to be given to an AI model. It serves as a set of instructions to the large language model.</li> </ul> </li> <li> <p>What is prompt engineering?</p> <ul> <li>The process of discovering prompts that yield desired results reliably. </li> </ul> </li> <li> <p>General issues in prompts given to AI models:</p> <ul> <li>Vague direction<ul> <li>Tell AI proper direction to take. Style emulation, language levels etc.</li> </ul> </li> <li>Unformatted output<ul> <li>Tell what kind of output is needed. For example csv.</li> </ul> </li> <li>Missing examples<ul> <li>Provide examples of what a god response looks like.</li> </ul> </li> <li>Limited evaluation<ul> <li>Setup a evaluation methodology to understand how the prompt is performing.</li> </ul> </li> <li>No task division<ul> <li>In many tasks there are multiple steps that need task specialization. This needs task specialization.</li> </ul> </li> </ul> </li> <li> <p>Tokens and Token optimization</p> <ul> <li>LLMs work by continuously predicting the next token starting from what was there in the prompt.</li> <li>A token is generally \u00beth of a word.</li> <li>Each new token is predicted based on the probablity of it appearing next.</li> <li>This is controlled by the temperature parameter.</li> <li>Average prompts give average responses.</li> <li>AI companies charge by the tokens i.e. the number of tokens used in the prompt and response.</li> <li>This means we need to optimize for tokens to get the best output in terms of cost, quality and reliability.</li> </ul> </li> </ul>"},{"location":"Technologies/prompt-engg/#five-principles-of-prompting","title":"Five principles of prompting","text":"Principle Description Give Direction Specify style or persona Specify format Define rules, structure of the response Provide examples Diverse set of tests that define task done successfully Evaluate quality Identify errors, rate responses, drive performance Divide labor chain tasks to achieve complex goals <p>These principles are model agnostic and work equally well for any type of response generation.</p> <ul> <li>For reference on how to apply these principles use the below 1 pager guides:<ul> <li>Text generation one-page prompt guide</li> <li>Image generation one-page prompt guide</li> </ul> </li> </ul>"},{"location":"Technologies/prompt-engg/#1-give-direction","title":"1. Give Direction","text":"<ul> <li>Human-like brief<ul> <li>For this step imagine what brief a human would require to work on the task. </li> <li>Using a similar brief to ask AI To do the task would do the trick most of the times.</li> <li>Although its not a 1-1 mapping</li> </ul> </li> <li>Role playing <ul> <li>E.g. ask AI to name a product how Elon musk or Steve jobs would do.</li> <li>Provide examples related to role playing that will set the tone for the response. E.g. provide product description and ex. product names</li> </ul> </li> <li>Pre warming or Internal retrieval<ul> <li>Start the conversation by asking for best practice advice.</li> <li>Ask AI to follow its own advice.</li> <li>This way you can ask AI to generate its own direction.</li> </ul> </li> <li>Best advice out there strategy<ul> <li>Take the best advice available and insert it into the prompt.</li> <li>This one can make the prompt longer but can improve the result significantly.</li> <li>Consider the cost tradeoffs though if using API for example.</li> </ul> </li> <li>Scale back<ul> <li>Sometimes too much instruction could be bad. It can restrict the creativity of the model.</li> <li>As there might not be enough training data out there to generate a response.</li> <li>So in this case scale back on some aspects of direction needed and try to get a desired response.</li> <li>Prioritize which elements are important.</li> </ul> </li> </ul>"},{"location":"Technologies/prompt-engg/#2-specify-format","title":"2. Specify format","text":"<ul> <li>AI models tend to change output formats for the same prompt given multiple times if not specified.</li> <li>Hence, to keep the output consistent its important we specify the format.</li> <li>This helps us to plan as well as we can use the output of the prompt for another step.</li> <li>For eg. if we specify a json output we can use it to render front end.</li> <li>Format can be specified the following ways:<ul> <li>One off: In the prompt text itself. Eg. Share a list in json format.</li> <li>Chain of prompts: Specify at the start and set as a persona. Eg. You are an assistant that responds in json.</li> <li>Setup in model parameters. Called grammars in llama models.</li> </ul> </li> <li>Remove other aspects from the prompt that might clash with specified format.</li> <li>\"The more layers of unrelated elements the more likely you are to get an unsuitable image.\"</li> <li>If there is a clash in the specify format principle and give direction principle lose the one less important.</li> </ul>"},{"location":"Technologies/prompt-engg/#3-provide-examples","title":"3. Provide examples","text":"<ul> <li>AI models are few shot learners.</li> <li>The reliability of an output increases tremendously if a few examples are provided instead of none.</li> <li>The tradeoff is creativity as more examples help provide constrains to the ask which reduces the creative index.</li> <li>Providing 3-5 examples will give a very reliable output at the cost of creativity.</li> <li>Providing 1-3 examples has a positive effect almost always.</li> <li>Above 3 consider the effect it might have and provide diverse examples to cover edge cases.</li> <li>Rule of thumb. Provide direction first then provide examples.</li> <li>For image generation providing base image is considered as giving an example.</li> </ul>"},{"location":"Technologies/prompt-engg/#4-evaluate-quality","title":"4. Evaluate quality","text":"<ul> <li>There is a need to evaluate quality when same prompt is relied on for production usecase.</li> <li>Having a simple thumbs-up/down system without adding much overhead to the optimization process.</li> <li>Another test is to see if examples are worth the additional cost as compared to giving no examples at all.</li> <li>Often while evaluating quality many elements of the prompt can be deemed superfluous or unnecessary.</li> <li>Removing them can shorten the prompt and improve latency resulting in overall scaling of operations.</li> <li>For images quality can be evaluated by permutation prompting.</li> <li>Provide a variety of input and do a side by side comparison / use classifiers for deciding the validity of each input.</li> </ul>"},{"location":"Technologies/prompt-engg/#5-divide-labour","title":"5. Divide labour","text":"<ul> <li>Just like solving an engineering problem prompt engineering problems can also be solved by decomposing them into smaller steps.</li> <li>The outputs of these small steps can be combined to form the eventual answer.</li> <li>AIs are unable to judge the quality of a response while responding.</li> <li>However, once they give the response the AIs can be asked to evaluate the same.</li> <li>So adding an extra step to evaluate the responses is key.</li> <li>Later we can chain to provide details and even ask AI to use the details to generate further tasks like creating an image generation prompt.</li> </ul>"},{"location":"Technologies/prompt-engg/#introduction-to-llms-for-text-generation","title":"Introduction to LLMs for text generation","text":""},{"location":"Technologies/prompt-engg/#text-generation-standard-practices","title":"Text Generation: standard practices","text":""},{"location":"Technologies/prompt-engg/#advanced-text-generation-with-langchain","title":"Advanced text generation with Langchain","text":""},{"location":"Technologies/prompt-engg/#vector-databases-with-faiss-and-pinecone","title":"Vector databases with FAISS and Pinecone","text":""},{"location":"Technologies/prompt-engg/#autonomous-agents-with-memory-tools","title":"Autonomous agents with memory &amp; tools","text":""},{"location":"Technologies/prompt-engg/#diffusion-models-for-image-generation","title":"Diffusion models for image generation","text":""},{"location":"Technologies/prompt-engg/#image-generation-standard-practices-with-mid-journey","title":"Image generation: Standard practices with mid-journey","text":""},{"location":"Technologies/prompt-engg/#image-generation-advanced-practices-with-stable-diffusion","title":"Image generation: Advanced practices with stable diffusion","text":""},{"location":"Technologies/prompt-engg/#building-ai-powered-applications","title":"Building AI-powered applications","text":""},{"location":"Technologies/snowflake/","title":"Introduction to Snowflake DB","text":""},{"location":"Technologies/snowflake/#overview","title":"Overview","text":"<ul> <li>Snowflake DB is a cloud-native SaaS data platform</li> <li>It provides data platform services with 0 install requirement</li> <li>It connects to all major cloud providers</li> <li>Uses columnar storage and follows a STAR schema</li> <li>Provides ways to aggregate data as well (average, sum, min, max etc.)</li> </ul>"},{"location":"Technologies/snowflake/#snowflake-data-platform","title":"Snowflake Data platform","text":""},{"location":"Technologies/snowflake/#date-warehouse-vs-data-lake","title":"Date warehouse vs Data lake","text":"Data Warehouse Data Lake Structured Data Unstructured Data (files) Tables Stored as objects Schema on read Schema on read Huge Volume Massive Volume <ul> <li>Data Lake house is a mix of these concepts</li> </ul>"},{"location":"Technologies/snowflake/#snowflake-data-warehousing-analytics-solution","title":"Snowflake - Data warehousing analytics solution","text":"Used for Not Used for Ad-hoc reads OLTP Data ware house reads Transactions Data lake reads Constraint enforcement <ul> <li>Key features:<ul> <li>Virtual, multi-cluster warehouse</li> <li>Time-travel and zero copy cloning</li> <li>Data sharing and marketplace</li> <li>High availability - cross cloud replication</li> <li>Granular security and automatic encryption</li> <li>Strong support for JSON</li> </ul> </li> </ul>"},{"location":"Technologies/snowflake/#snowflake-tools","title":"Snowflake tools","text":"<p>The tools used to interact and use Snowflake:     - Web UI and console     - SnowSQL - Python based CLI     - Snowpipe - Streaming based ingest     - Drivers and SDK for other languages     - Partner connections</p>"},{"location":"Technologies/snowflake/#snowflake-storage","title":"Snowflake Storage","text":"<ul> <li> <p>Snowflake storage is:</p> <ul> <li>On cloud blob storage or file storage</li> <li>Scalable and available</li> <li>Encrypted</li> <li>Compressed</li> <li>Auto-partitioned</li> </ul> </li> <li> <p>Ways to load data:</p> <ul> <li>External Table</li> <li>Copy command<ul> <li>Large single shot copies</li> </ul> </li> <li>SnowPipe object<ul> <li>Micro copies. continuous entries</li> </ul> </li> </ul> </li> <li> <p>Data load process:</p> </li> </ul> <pre><code>graph LR\nA([Source from bucket]) --&gt; B([Create file format]);\nB --&gt; | &lt;= 100MB|C([Load from WebUI]);\nB --&gt; |PUT command| D([CLI]);\nC --&gt; E([Staging Table]);\nD --&gt; E;\nE --&gt; |Default|F[Permanent Storage]\nE --&gt; G[Temporary Storage]\nE --&gt; H[Transient Storage]\nE --&gt; |Data Lakes|I[External]</code></pre>"}]}